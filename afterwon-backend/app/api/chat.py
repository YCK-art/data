from fastapi import APIRouter, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import List, Optional, Any, Dict
import json
import asyncio
from ..services.ai_service import AIService
from ..services.file_service import FileService

router = APIRouter(prefix="/api/chat", tags=["chat"])

# ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
ai_service = AIService()
file_service = FileService()

class ConversationMessage(BaseModel):
    role: str  # 'user' or 'assistant'
    content: str
    timestamp: Optional[str] = None

class UnifiedQuestionRequest(BaseModel):
    question: str
    file_id: Optional[str] = None  # íŒŒì¼ì´ ìˆìœ¼ë©´ file_id ì œê³µ
    conversation_history: Optional[List[ConversationMessage]] = None  # ëŒ€í™” íˆìŠ¤í† ë¦¬

class UnifiedQuestionResponse(BaseModel):
    answer: str
    codeExecution: Optional[Dict[str, Any]] = None
    insights: Optional[List[str]] = None
    followUpQuestions: Optional[List[str]] = None
    chartData: Optional[Any] = None
    fileInfo: Optional[Dict[str, Any]] = None

class GeneralQuestionRequest(BaseModel):
    question: str

class GeneralQuestionResponse(BaseModel):
    answer: str

class ChatTitleRequest(BaseModel):
    message: str

class ChatTitleResponse(BaseModel):
    title: str

@router.post("/unified-question", response_model=UnifiedQuestionResponse)
async def ask_unified_question(request: UnifiedQuestionRequest):
    """
    í†µí•©ëœ ì§ˆë¬¸ ì²˜ë¦¬ - íŒŒì¼ ìœ ë¬´ì— ê´€ê³„ì—†ì´ ì½”ë“œ ì‹¤í–‰ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ ë‹µë³€ ì œê³µ
    """
    try:
        df = None
        file_info = None

        # íŒŒì¼ì´ ìˆëŠ” ê²½ìš° ë°ì´í„° ë¡œë“œ
        if request.file_id:
            try:
                print(f"ğŸ” Attempting to load file: {request.file_id}")
                df = file_service.get_dataframe(request.file_id)
                file_metadata = file_service.get_file_metadata(request.file_id)

                if df is None:
                    print(f"âŒ Failed to load DataFrame for file_id: {request.file_id}")
                    print(f"ğŸ“‹ Available files: {list(file_service.file_metadata.keys())}")
                elif file_metadata is None:
                    print(f"âŒ No metadata found for file_id: {request.file_id}")
                else:
                    file_info = {
                        'filename': file_metadata.get('filename', 'Unknown'),
                        'fileSize': file_metadata.get('file_size', 0),
                        'fileType': file_metadata.get('file_type', 'unknown'),
                        'file_id': request.file_id
                    }
                    print(f"ğŸ“Š Loaded data: {len(df)} rows, {len(df.columns)} columns")
                    print(f"ğŸ“ File info: {file_info}")

            except Exception as e:
                print(f"âŒ File loading error: {str(e)}")
                import traceback
                print(f"Full traceback: {traceback.format_exc()}")
                # íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨í•´ë„ ì¼ë°˜ ì§ˆë¬¸ìœ¼ë¡œ ì²˜ë¦¬
                pass

        # í†µí•© ë¶„ì„ ì‹¤í–‰ (ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬í•¨)
        result = await ai_service.unified_analysis(request.question, df, file_info, request.conversation_history)

        return UnifiedQuestionResponse(
            answer=result.get('answer', ''),
            codeExecution=result.get('code_execution'),
            insights=result.get('insights'),
            followUpQuestions=result.get('followUpQuestions'),
            chartData=result.get('chartData'),
            fileInfo=file_info
        )

    except Exception as e:
        print(f"Unified question error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@router.post("/question", response_model=GeneralQuestionResponse)
async def ask_general_question(request: GeneralQuestionRequest):
    """
    íŒŒì¼ ì—†ì´ ì¼ë°˜ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤. (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)
    """
    try:
        # í†µí•© ë¶„ì„ ì‹œìŠ¤í…œ ì‚¬ìš©
        result = await ai_service.unified_analysis(request.question)

        return GeneralQuestionResponse(answer=result.get('answer', ''))

    except Exception as e:
        print(f"General question error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@router.post("/unified-question-stream")
async def ask_unified_question_stream(request: UnifiedQuestionRequest):
    """
    í†µí•©ëœ ì§ˆë¬¸ ì²˜ë¦¬ - ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì‹¤ì‹œê°„ ì‘ë‹µ ì œê³µ
    """
    async def generate_response():
        try:
            df = None
            file_info = None

            # íŒŒì¼ì´ ìˆëŠ” ê²½ìš° ë°ì´í„° ë¡œë“œ
            if request.file_id:
                try:
                    print(f"ğŸ” Attempting to load file: {request.file_id}")
                    df = file_service.get_dataframe(request.file_id)
                    file_metadata = file_service.get_file_metadata(request.file_id)

                    if df is None:
                        print(f"âŒ Failed to load DataFrame for file_id: {request.file_id}")
                        print(f"ğŸ“‹ Available files: {list(file_service.file_metadata.keys())}")
                    elif file_metadata is None:
                        print(f"âŒ No metadata found for file_id: {request.file_id}")
                    else:
                        file_info = {
                            'filename': file_metadata.get('filename', 'Unknown'),
                            'fileSize': file_metadata.get('file_size', 0),
                            'fileType': file_metadata.get('file_type', 'unknown'),
                            'file_id': request.file_id
                        }
                        print(f"ğŸ“Š Loaded data: {len(df)} rows, {len(df.columns)} columns")
                        print(f"ğŸ“ File info: {file_info}")

                except Exception as e:
                    print(f"âŒ File loading error: {str(e)}")
                    import traceback
                    print(f"Full traceback: {traceback.format_exc()}")

            # ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„ ì‹¤í–‰
            print(f"ğŸ”„ generate_responseì—ì„œ unified_analysis_stream í˜¸ì¶œ ì „")
            try:
                # íƒ€ì„ì•„ì›ƒ ì„¤ì • (180ì´ˆ = 3ë¶„)
                stream_timeout = 180
                start_time = asyncio.get_event_loop().time()

                async for chunk in ai_service.unified_analysis_stream(request.question, df, file_info, request.conversation_history):
                    # íƒ€ì„ì•„ì›ƒ ì²´í¬
                    if asyncio.get_event_loop().time() - start_time > stream_timeout:
                        print(f"â° ìŠ¤íŠ¸ë¦¬ë° íƒ€ì„ì•„ì›ƒ (180ì´ˆ ì´ˆê³¼)")
                        timeout_chunk = {
                            "type": "error",
                            "content": "ì‘ë‹µ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
                        }
                        yield f"data: {json.dumps(timeout_chunk, ensure_ascii=False)}\n\n"
                        break

                    print(f"ğŸ“¦ chunk ë°›ìŒ: {chunk.get('type', 'unknown')}")
                    yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"
                print(f"âœ… ëª¨ë“  chunk ì²˜ë¦¬ ì™„ë£Œ")
            except Exception as stream_error:
                print(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜: {str(stream_error)}")
                import traceback
                print(f"ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜ íŠ¸ë ˆì´ìŠ¤ë°±: {traceback.format_exc()}")
                # ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ í´ë¼ì´ì–¸íŠ¸ì— ì „ì†¡
                error_chunk = {
                    "type": "error",
                    "content": f"ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {str(stream_error)}"
                }
                yield f"data: {json.dumps(error_chunk, ensure_ascii=False)}\n\n"

        except Exception as e:
            print(f"Streaming unified question error: {str(e)}")
            error_chunk = {
                "type": "error",
                "content": f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            }
            yield f"data: {json.dumps(error_chunk, ensure_ascii=False)}\n\n"

    return StreamingResponse(
        generate_response(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Content-Type": "text/event-stream; charset=utf-8",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Methods": "GET, POST, OPTIONS",
            "Access-Control-Allow-Headers": "Content-Type"
        }
    )

@router.post("/generate-title", response_model=ChatTitleResponse)
async def generate_chat_title(request: ChatTitleRequest):
    """
    ì‚¬ìš©ìì˜ ì²« ë©”ì‹œì§€ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì±„íŒ… ì œëª©ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    try:
        # AI ì„œë¹„ìŠ¤ë¥¼ í†µí•´ ì±„íŒ… ì œëª© ìƒì„±
        title = await ai_service.generate_chat_title(request.message)

        return ChatTitleResponse(title=title)

    except Exception as e:
        print(f"Chat title generation error: {str(e)}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ì œëª© ë°˜í™˜
        return ChatTitleResponse(title="ìƒˆ ì±„íŒ…")