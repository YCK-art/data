import pandas as pd
import numpy as np
import re
import httpx
import json
import asyncio
import matplotlib
matplotlib.use('Agg')  # ë¸Œë¼ìš°ì € ìžë™ ì—´ê¸° ë°©ì§€
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
import plotly
import plotly.io as pio
pio.renderers.default = "json"  # Plotly ë¸Œë¼ìš°ì € ìžë™ ì—´ê¸° ë°©ì§€
import io
import sys
from contextlib import redirect_stdout, redirect_stderr
from openai import AsyncOpenAI
from typing import Dict, Any, List
from ..core.config import settings

def convert_numpy_types(obj):
    """NumPy/pandas íƒ€ìž…ì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ Python íƒ€ìž…ìœ¼ë¡œ ë³€í™˜"""
    import pandas as pd
    from datetime import datetime

    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, (pd.Timestamp, pd.Timedelta)):
        return str(obj)  # Timestampë¥¼ ë¬¸ìžì—´ë¡œ ë³€í™˜
    elif isinstance(obj, datetime):
        return obj.isoformat()  # datetimeì„ ISO í˜•ì‹ ë¬¸ìžì—´ë¡œ ë³€í™˜
    elif isinstance(obj, pd.Series):
        return obj.tolist()  # Seriesë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    elif hasattr(obj, '__array__'):  # pandas ê´€ë ¨ ë°°ì—´ íƒ€ìž…ë“¤
        try:
            return obj.tolist()
        except:
            return str(obj)
    elif hasattr(obj, 'item'):  # NumPy scalar
        return obj.item()
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    else:
        # ë§ˆì§€ë§‰ fallback: JSON ì§ë ¬í™” í…ŒìŠ¤íŠ¸
        try:
            import json
            json.dumps(obj)
            return obj
        except (TypeError, ValueError):
            # JSON ì§ë ¬í™” ì‹¤íŒ¨ì‹œ ë¬¸ìžì—´ë¡œ ë³€í™˜
            return str(obj)

class AIService:
    def __init__(self):
        self.client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
    
    async def analyze_data(self, df: pd.DataFrame, question: str, eda_data: Dict[str, Any] = None) -> Dict[str, Any]:
        print(f"ðŸš€ analyze_data called with question: '{question}'")
        """ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì¸ì‚¬ì´íŠ¸ ë° ì°¨íŠ¸ ì •ë³´ ë°˜í™˜"""
        
        # Provide comprehensive data analysis to AI - OPTIMIZED for large datasets
        if eda_data:
            data_info = self._format_eda_for_ai(eda_data)
            # Limit sample data to prevent token overflow
            preview_data = eda_data.get("preview", {}).get("head", convert_numpy_types(df.head(3).to_dict()))
            data_sample = str(preview_data)[:2000]  # Truncate long samples
        else:
            data_info = self._get_data_info(df)
            # Use smaller sample and truncate for large datasets
            sample_size = 3 if len(df) > 1000 else 5
            data_sample = str(convert_numpy_types(df.head(sample_size).to_dict()))[:2000]  # Truncate long samples
            
        # CRITICAL: Provide actual data statistics and aggregations to match chart generation
        data_statistics = self._get_comprehensive_data_stats(df, question)
        
        # ì‹¤ì œ ì»¬ëŸ¼ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        available_columns = df.columns.tolist()
        
        # Analyze user intent and map to appropriate columns
        intent_analysis = self._analyze_user_intent(question, available_columns)
        suggested_columns = intent_analysis['suggested_columns']
        analysis_focus = intent_analysis['analysis_focus']
        
        # OPTIMIZED analysis prompt - condensed for large datasets
        analysis_prompt = f"""
Data analyst task: Answer "{question}" using the dataset below.

DATASET: {len(df):,} records, {len(df.columns)} columns
KEY STATS: {data_statistics}
SAMPLE: {data_sample}
COLUMNS: {available_columns}

ANALYSIS FOCUS: {analysis_focus}
KEY COLUMNS: {suggested_columns}

CHART TYPE RULES (COMPREHENSIVE PLOTLY SUPPORT):
BASIC: "pie/line/bar/scatter/area" â†’ use respective type
STATISTICAL: "histogram/box/violin/strip/density_contour/density_heatmap/distplot/ecdf"
SPECIALIZED: "funnel/waterfall/treemap/sunburst/radar/heatmap"
GEO/MAP: "choropleth/scattergeo" â†’ for geographic data (countries, regions, coordinates)
3D: "scatter_3d/surface/line_3d/mesh3d"
FINANCIAL: "candlestick/ohlc"
MULTIVARIATE: "parallel_coordinates/parallel_categories"
KOREAN SUPPORT: ížˆìŠ¤í† ê·¸ëž¨/ë°•ìŠ¤í”Œë¡¯/ë°”ì´ì˜¬ë¦°/íŠ¸ë¦¬ë§µ/ì„ ë²„ìŠ¤íŠ¸/ë ˆì´ë”/ížˆíŠ¸ë§µ/ê¹”ë•Œê¸°/í­í¬ì°¨íŠ¸/ì‚°í¬ë„/3Dì‚°ì ë„/í‘œë©´ì°¨íŠ¸/ìº”ë“¤ìŠ¤í‹±/í‰í–‰ì¢Œí‘œ/ë¶„í¬í”Œë¡¯/ëˆ„ì ë¶„í¬/ì§€ë„ì°¨íŠ¸/ì§€ë¦¬ì ì‚°ì ë„
GEOGRAPHIC KEYWORDS: ì§€ë„/ì§€ì—­/êµ­ê°€/ì‹œë„/ìœ„ì¹˜/ì¢Œí‘œ/latitude/longitude/country/region/map/ì§€ë„ì°¨íŠ¸ â†’ use "choropleth" or "scattergeo"
Default: proportionsâ†’pie, trendsâ†’line, comparisonsâ†’bar, correlationsâ†’scatter, distributionsâ†’histogram, hierarchicalâ†’treemap, financialâ†’candlestick, geographicâ†’choropleth

RESPONSE FORMAT (JSON only):
{{
  "insights": [
    "## í•µì‹¬ ë°œê²¬ì‚¬í•­\n- [í•µì‹¬ ë°œê²¬ì‚¬í•­ 1 - êµ¬ì²´ì ì´ê³  ëª…í™•í•˜ê²Œ]\n- [í•µì‹¬ ë°œê²¬ì‚¬í•­ 2 - ìˆ˜ì¹˜ì™€ í•¨ê»˜]\n- [í•µì‹¬ ë°œê²¬ì‚¬í•­ 3 - íŒ¨í„´ ì„¤ëª…]",
    "### ì£¼ìš” í†µê³„\n- ì „ì²´ ë°ì´í„° ìˆ˜: [ìˆ«ìž]ê°œ\n- í•µì‹¬ ì§€í‘œ: [êµ¬ì²´ì  ìˆ˜ì¹˜ì™€ í•¨ê»˜ ì„¤ëª…]\n- í‰ê· ê°’: [í‰ê· ]\n- ìµœëŒ“ê°’/ìµœì†Ÿê°’: [ë²”ìœ„ ì •ë³´]\n- ë¶„í¬ íŠ¹ì„±: [ë¶„í¬ì— ëŒ€í•œ ì„¤ëª…]",
    "### ì„¸ë¶€ ë¶„ì„\n- [íŒ¨í„´ 1]: [êµ¬ì²´ì  ì„¤ëª…ê³¼ ìˆ˜ì¹˜]\n- [íŒ¨í„´ 2]: [êµ¬ì²´ì  ì„¤ëª…ê³¼ ìˆ˜ì¹˜]\n- [íŠ¸ë Œë“œ]: [ì‹œê°„ë³„/ì¹´í…Œê³ ë¦¬ë³„ ë³€í™” ì–‘ìƒ]\n- [ìƒê´€ê´€ê³„]: [ë³€ìˆ˜ë“¤ ê°„ì˜ ê´€ê³„ì™€ ê°•ë„]",
    "### ì‹¤í–‰ ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸\n- [ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ì  1]: [êµ¬ì²´ì  ì˜ë¯¸ì™€ ì˜í–¥]\n- [ê°œì„ ë°©ì•ˆ]: [ì‹¤í–‰ ê°€ëŠ¥í•œ ì œì•ˆ]\n- [ë‹¤ìŒ ë¶„ì„ ë°©í–¥]: [ì¶”ê°€ ë¶„ì„ ì œì•ˆ]"
  ],
  "chart_type": "bar|line|pie|scatter|histogram|box|violin|treemap|sunburst|radar|heatmap|choropleth|scattergeo",
  "chart_columns": {{"x": "exact_column_name", "y": "exact_column_name"}},
  "summary": "{question}ì— ëŒ€í•œ ê°„ë‹¨ëª…ë£Œí•œ ë‹µë³€. ì¸ì‚¬ì´íŠ¸ì™€ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ì§ì ‘ì ì¸ ë‹µë³€ë§Œ ì œê³µ.",
  "follow_up_questions": [
    "Follow-up question 1",
    "Follow-up question 2", 
    "Follow-up question 3"
  ]
}}
"""
        
        try:
            # Check if API key is properly set
            if not settings.OPENAI_API_KEY or settings.OPENAI_API_KEY == "your_openai_api_key_here":
                raise ValueError("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
            response = await self.client.chat.completions.create(
                model="gpt-4o",  # Use more powerful model for complex analysis
                messages=[
                    {"role": "system", "content": "You are a senior data analyst. Provide two distinct types of content: 1) 'summary': A brief, direct answer to the user's question (2-3 sentences max, no bullet points, no markdown headers). 2) 'insights': Detailed analysis using markdown headings (## ###) and bullet points (-) with specific findings, statistics, and recommendations. For mathematical expressions, use LaTeX notation enclosed in $ for inline math (like $\\frac{a}{b}$) or $$ for display math (like $$\\frac{numerator}{denominator}$$). Use proper LaTeX for fractions, square roots, exponents, etc. Ensure NO overlap between summary and insights content. Keep insights concise but comprehensive. Always complete your response - never truncate content."},
                    {"role": "user", "content": analysis_prompt}
                ],
                temperature=0.1,
                max_tokens=4000  # Allow more comprehensive responses without truncation
            )
            
            # JSON ì‘ë‹µ íŒŒì‹±
            raw_content = response.choices[0].message.content
            
            try:
                result = json.loads(raw_content)
                print(f"ðŸ“ JSON parsed result chart_type: {result.get('chart_type', 'N/A')}")
                # Ensure backward compatibility with old structure
                if "insights" not in result and "data_quality" in result:
                    # Convert new structure to backward compatible format
                    converted = self._convert_comprehensive_analysis(result)
                    print(f"ðŸ“ Converted result chart_type: {converted.get('chart_type', 'N/A')}")
                    return converted
                print(f"ðŸ“ Returning direct result with chart_type: {result.get('chart_type', 'N/A')}")

                # Use OpenAI's results as-is (no modifications)
                print(f"ðŸ”§ Using OpenAI results as-is:")
                print(f"   Chart Type: {result.get('chart_type')}")
                print(f"   Chart Columns: {result.get('chart_columns')}")
                print(f"   Insights: {len(result.get('insights', []))} items")

                return convert_numpy_types(result)
            except json.JSONDecodeError:
                # If JSON parsing fails, try to extract JSON from the content
                result = self._extract_json_from_response(raw_content)
                if result:
                    return convert_numpy_types(result)
                # If no JSON found, create conversational response
                return convert_numpy_types(self._create_conversational_analysis(df, question, raw_content))
            
        except Exception as e:
            # ë” êµ¬ì²´ì ì¸ ì˜¤ë¥˜ ë©”ì‹œì§€ ì œê³µ
            error_msg = str(e)
            if "API key" in error_msg.lower() or "authentication" in error_msg.lower():
                insights = [
                    f"ë°ì´í„°ì— {len(df)} ê°œì˜ í–‰ê³¼ {len(df.columns)} ê°œì˜ ì—´ì´ ìžˆìŠµë‹ˆë‹¤.",
                    "âš ï¸ OpenAI API í‚¤ë¥¼ .env íŒŒì¼ì— ì„¤ì •í•´ì£¼ì„¸ìš”.",
                    "í˜„ìž¬ëŠ” ê¸°ë³¸ í†µê³„ ì •ë³´ë§Œ ì œê³µí•©ë‹ˆë‹¤."
                ]
                summary = "OpenAI API í‚¤ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
            else:
                insights = [
                    f"ë°ì´í„°ì— {len(df)} ê°œì˜ í–‰ê³¼ {len(df.columns)} ê°œì˜ ì—´ì´ ìžˆìŠµë‹ˆë‹¤.",
                    f"ì˜¤ë¥˜ ë°œìƒ: {error_msg}",
                    "ê¸°ë³¸ ë°ì´í„° ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤."
                ]
                summary = f"AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {error_msg}"
            
            # Use intent analysis for smart column selection in error cases too
            intent_analysis = self._analyze_user_intent(question, df.columns.tolist())
            x_col = intent_analysis['suggested_columns']['x']
            y_col = intent_analysis['suggested_columns']['y']
            
            # Generate follow-up questions even in error cases
            follow_up_questions = self._generate_follow_up_questions(question, df.columns.tolist(), intent_analysis)
            
            return convert_numpy_types({
                "insights": insights,
                "chart_type": intent_analysis['chart_type'],
                "chart_columns": {"x": x_col, "y": y_col},
                "summary": summary,
                "follow_up_questions": follow_up_questions
            })
    
    def _get_data_info(self, df: pd.DataFrame) -> str:
        """ë°ì´í„° ê¸°ë³¸ ì •ë³´ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
        info = []
        info.append(f"í–‰ ìˆ˜: {len(df)}")
        info.append(f"ì—´ ìˆ˜: {len(df.columns)}")
        info.append(f"ì»¬ëŸ¼ëª…: {', '.join(df.columns.tolist())}")
        
        # ìˆ«ìží˜• ì»¬ëŸ¼ í†µê³„
        numeric_cols = df.select_dtypes(include=['number']).columns
        if len(numeric_cols) > 0:
            info.append(f"ìˆ«ìží˜• ì»¬ëŸ¼: {', '.join(numeric_cols.tolist())}")
        
        return "; ".join(info)
    
    def _format_eda_for_ai(self, eda_data: Dict[str, Any]) -> str:
        """EDA ê²°ê³¼ë¥¼ AIê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ í˜•íƒœë¡œ í¬ë§·íŒ…"""
        info = []
        
        # ê¸°ë³¸ ì •ë³´
        basic = eda_data.get("basic_info", {})
        if basic:
            info.append(f"ë°ì´í„° í¬ê¸°: {basic.get('shape', 'N/A')}")
            info.append(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {basic.get('memory_usage', 'N/A')}")
        
        # ì»¬ëŸ¼ íƒ€ìž… ì •ë³´
        col_types = eda_data.get("column_types", {})
        if col_types:
            info.append(f"ìˆ«ìží˜• ì»¬ëŸ¼ {col_types.get('numeric_count', 0)}ê°œ: {', '.join(col_types.get('numeric', []))}")
            info.append(f"ë²”ì£¼í˜• ì»¬ëŸ¼ {col_types.get('categorical_count', 0)}ê°œ: {', '.join(col_types.get('categorical', []))}")
            if col_types.get('datetime_count', 0) > 0:
                info.append(f"ë‚ ì§œí˜• ì»¬ëŸ¼ {col_types.get('datetime_count', 0)}ê°œ")
        
        # ë°ì´í„° í’ˆì§ˆ ì •ë³´
        quality = eda_data.get("data_quality", {})
        if quality:
            info.append(f"ë°ì´í„° ì™„ì„±ë„: {quality.get('completeness', 0)}%")
            info.append(f"ë°ì´í„° ê³ ìœ ì„±: {quality.get('uniqueness', 0)}%")
        
        # ê²°ì¸¡ê°’ ì •ë³´
        missing = eda_data.get("missing_data", {})
        if missing.get("total_missing", 0) > 0:
            info.append(f"ê²°ì¸¡ê°’: {missing.get('total_missing', 0)}ê°œ")
            missing_cols = list(missing.get("missing_by_column", {}).keys())
            if missing_cols:
                info.append(f"ê²°ì¸¡ê°’ ìžˆëŠ” ì»¬ëŸ¼: {', '.join(missing_cols[:3])}")
        
        # ì¤‘ë³µ ë°ì´í„° ì •ë³´
        duplicates = eda_data.get("duplicates", {})
        if duplicates.get("duplicate_rows", 0) > 0:
            info.append(f"ì¤‘ë³µ í–‰: {duplicates.get('duplicate_rows', 0)}ê°œ ({duplicates.get('duplicate_percentage', 0)}%)")
        
        # ì¶”ì²œì‚¬í•­
        recommendations = eda_data.get("recommendations", [])
        if recommendations:
            info.append(f"ì¶”ì²œ ë¶„ì„: {', '.join(recommendations[:3])}")
        
        return "; ".join(info)
    
    def _get_comprehensive_data_stats(self, df: pd.DataFrame, question: str) -> str:
        """Generate OPTIMIZED data statistics that match what charts will show - REDUCED for large datasets"""
        stats = []
        
        # Get basic info
        stats.append(f"Total records: {len(df):,}")
        stats.append(f"Total columns: {len(df.columns)}")
        
        # OPTIMIZATION: Limit processing for large datasets to avoid token limits
        max_categorical_cols = 3  # Limit categorical analysis to prevent token overflow
        max_numeric_cols = 5      # Limit numeric analysis
        max_categories_per_col = 5  # Limit categories shown per column
        
        # Analyze categorical columns with value counts (what pie/bar charts will show)
        categorical_columns = df.select_dtypes(include=['object', 'category']).columns[:max_categorical_cols]
        
        for col in categorical_columns:
            if df[col].nunique() <= 50:  # Only for manageable categories, increased limit
                value_counts = df[col].value_counts()
                percentages = (df[col].value_counts(normalize=True) * 100).round(2)
                
                stats.append(f"\n{col} distribution (top {max_categories_per_col}):")
                for value, count in value_counts.head(max_categories_per_col).items():
                    percentage = percentages[value]
                    # NumPy íƒ€ìž…ì„ Python íƒ€ìž…ìœ¼ë¡œ ë³€í™˜
                    count = convert_numpy_types(count)
                    percentage = convert_numpy_types(percentage)
                    stats.append(f"  - {value}: {count:,} ({percentage}%)")
                    
                # Add summary for remaining categories if any
                if len(value_counts) > max_categories_per_col:
                    remaining = len(value_counts) - max_categories_per_col
                    stats.append(f"  - ... and {remaining} other categories")
        
        # Analyze numeric columns with basic statistics - LIMITED
        numeric_columns = df.select_dtypes(include=['int64', 'float64', 'int32', 'float32']).columns[:max_numeric_cols]
        
        for col in numeric_columns:
            if not df[col].isna().all():
                mean_val = convert_numpy_types(df[col].mean())
                min_val = convert_numpy_types(df[col].min())
                max_val = convert_numpy_types(df[col].max())
                count_val = convert_numpy_types(df[col].count())
                stats.append(f"\n{col} stats: Mean={mean_val:.2f}, Min={min_val}, Max={max_val}, Count={count_val:,}")
        
        # Skip correlations for very large datasets to save tokens
        if len(df) < 5000 and len(numeric_columns) >= 2:
            try:
                corr_matrix = df[numeric_columns].corr()
                stats.append(f"\nKey correlations:")
                # Get strongest correlations - limit to top 3
                correlations = []
                for i in range(len(numeric_columns)):
                    for j in range(i+1, len(numeric_columns)):
                        corr_val = convert_numpy_types(corr_matrix.iloc[i, j])
                        if abs(corr_val) > 0.3:  # Only significant correlations
                            correlations.append((abs(corr_val), f"{numeric_columns[i]} vs {numeric_columns[j]}: {corr_val:.3f}"))
                
                # Sort by strength and take top 3
                correlations.sort(reverse=True)
                for _, corr_text in correlations[:3]:
                    stats.append(f"  - {corr_text}")
            except:
                pass  # Skip if correlation fails
        
        # OPTIMIZED: Limit question-specific analysis to prevent token overflow
        question_lower = question.lower()
        question_matches = 0
        max_question_matches = 2  # Limit to 2 question-specific matches
        
        for col in df.columns[:5]:  # Only check first 5 columns
            if question_matches >= max_question_matches:
                break
                
            col_values = df[col].astype(str).str.lower()
            for word in question_lower.split()[:3]:  # Only check first 3 words
                if question_matches >= max_question_matches:
                    break
                    
                if len(word) > 2 and word in col_values.values:
                    matching_count = (col_values == word).sum()
                    total_count = len(df)
                    percentage = (matching_count / total_count * 100)
                    stats.append(f"\n'{word}' in {col}: {matching_count:,} ({percentage:.2f}%)")
                    question_matches += 1
        
        result = "; ".join(stats)
        
        # FINAL SAFEGUARD: Truncate if still too long
        if len(result) > 8000:  # Approximately 2000 tokens
            result = result[:8000] + "... (truncated for token limit)"
            
        return result
    
    def _generate_follow_up_questions(self, question: str, available_columns: List[str], intent_analysis: Dict[str, Any]) -> List[str]:
        """Generate contextual follow-up questions based on current analysis"""
        follow_ups = []
        question_lower = question.lower()
        
        # Get column types for smarter suggestions
        categorical_suggestions = []
        numeric_suggestions = []
        
        for col in available_columns:
            # Guess column types based on names (simple heuristic)
            col_lower = col.lower()
            if any(term in col_lower for term in ['country', 'region', 'type', 'category', 'status', 'method']):
                categorical_suggestions.append(col)
            elif any(term in col_lower for term in ['amount', 'value', 'price', 'count', 'number', 'rate']):
                numeric_suggestions.append(col)
        
        # Chart type specific follow-ups
        chart_type = intent_analysis.get('chart_type', 'bar')
        
        if chart_type == 'pie':
            # For pie charts, suggest comparisons and drill-downs
            follow_ups.extend([
                f"ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ë³„ë¡œë„ ë¶„ì„í•´ì£¼ì„¸ìš”",
                f"ì´ ë°ì´í„°ë¥¼ ë§‰ëŒ€ ì°¨íŠ¸ë¡œë„ ë³´ì—¬ì£¼ì„¸ìš”",
                f"ìƒìœ„ 5ê°œ í•­ëª©ë§Œ ë”°ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”"
            ])
        elif chart_type == 'bar':
            # For bar charts, suggest trends and correlations
            follow_ups.extend([
                f"ì´ ë°ì´í„°ì˜ íŠ¸ë Œë“œë¥¼ ì„  ê·¸ëž˜í”„ë¡œ ë³´ì—¬ì£¼ì„¸ìš”",
                f"ë¹„ìœ¨ë¡œ íŒŒì´ ì°¨íŠ¸ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”",
                f"í‰ê· ê°’ê³¼ ë¹„êµí•´ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”"
            ])
        elif chart_type == 'line':
            # For line charts, suggest correlations and predictions
            follow_ups.extend([
                f"ì´ íŠ¸ë Œë“œì˜ ì›ì¸ì„ ë¶„ì„í•´ì£¼ì„¸ìš”",
                f"ë‹¤ë¥¸ ë³€ìˆ˜ì™€ì˜ ìƒê´€ê´€ê³„ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”",
                f"ê³„ì ˆì„± íŒ¨í„´ì´ ìžˆëŠ”ì§€ ë¶„ì„í•´ì£¼ì„¸ìš”"
            ])
        elif chart_type == 'scatter':
            # For scatter plots, suggest deeper correlations
            follow_ups.extend([
                f"ìƒê´€ê´€ê³„ì˜ ê°•ë„ë¥¼ ìˆ˜ì¹˜ë¡œ ë³´ì—¬ì£¼ì„¸ìš”",
                f"ì´ìƒì¹˜(outlier)ë¥¼ ì‹ë³„í•´ì£¼ì„¸ìš”",
                f"íšŒê·€ ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”"
            ])
        
        # Content-based follow-ups
        if any(term in question_lower for term in ['country', 'êµ­ê°€', 'region']):
            follow_ups.append("ì§€ì—­ë³„ ì„±ê³¼ë¥¼ ì‹œê°„ì— ë”°ë¼ ë¶„ì„í•´ì£¼ì„¸ìš”")
            follow_ups.append("ê°€ìž¥ ì„±ê³¼ê°€ ì¢‹ì€/ë‚˜ìœ ì§€ì—­ì€ ì–´ë””ì¸ê°€ìš”?")
        
        if any(term in question_lower for term in ['ì‹¤íŒ¨', 'fail', 'error', 'problem']):
            follow_ups.append("ì‹¤íŒ¨ ì›ì¸ë³„ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”")
            follow_ups.append("ì‹¤íŒ¨ìœ¨ì´ ë†’ì€ ì‹œê°„ëŒ€ëŠ” ì–¸ì œì¸ê°€ìš”?")
        
        if any(term in question_lower for term in ['ë§¤ì¶œ', 'revenue', 'sales', 'amount']):
            follow_ups.append("ì›”ë³„ ë§¤ì¶œ ì„±ìž¥ë¥ ì„ ê³„ì‚°í•´ì£¼ì„¸ìš”")
            follow_ups.append("ë§¤ì¶œ êµ¬ê°„ë³„ ê³ ê° ë¶„í¬ë¥¼ ë³´ì—¬ì£¼ì„¸ìš”")
        
        # Column-specific suggestions
        if categorical_suggestions:
            follow_ups.append(f"{categorical_suggestions[0]} ë³„ ìƒì„¸ ë¶„ì„ì„ í•´ì£¼ì„¸ìš”")
        
        if numeric_suggestions:
            follow_ups.append(f"{numeric_suggestions[0]}ì˜ í†µê³„ì  ë¶„í¬ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”")
        
        # General analytical follow-ups
        follow_ups.extend([
            "ì´ ê²°ê³¼ì—ì„œ ê°€ìž¥ ì¤‘ìš”í•œ ì¸ì‚¬ì´íŠ¸ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
            "ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ì ì—ì„œ ì–´ë–¤ ì•¡ì…˜ì„ ì·¨í•´ì•¼ í• ê¹Œìš”?",
            "ì´ìƒ íŒ¨í„´ì´ë‚˜ íŠ¹ì´ì‚¬í•­ì´ ìžˆë‚˜ìš”?"
        ])
        
        # Remove duplicates and limit to 4 questions
        unique_follow_ups = list(dict.fromkeys(follow_ups))  # Preserves order while removing duplicates
        return unique_follow_ups[:4]
    
    def _analyze_user_intent(self, question: str, available_columns: List[str]) -> Dict[str, Any]:
        """Advanced context-aware intent analysis with deep semantic understanding"""
        question_lower = question.lower()
        
        # Enhanced semantic mappings with business context
        semantic_mappings = {
            # Payment/Transaction failures
            'failure': ['failure', 'error', 'fail', 'failed', 'failing', 'problem', 'issue', 'decline', 'declined', 'reject', 'rejected'],
            'success': ['success', 'successful', 'complete', 'completed', 'approved', 'passed', 'accept', 'accepted'],
            'reason': ['reason', 'message', 'cause', 'why', 'explanation', 'details', 'description', 'info'],
            'code': ['code', 'status_code', 'error_code', 'response_code', 'result_code'],
            
            # Card/Payment methods
            'card': ['card', 'payment_method', 'payment', 'credit', 'debit', 'visa', 'mastercard', 'amex', 'discover'],
            'network': ['network', 'type', 'brand', 'issuer', 'provider', 'company'],
            'country': ['country', 'location', 'region', 'nation', 'geography', 'address', 'origin'],
            
            # Customer/User analysis
            'customer': ['customer', 'user', 'client', 'account', 'holder', 'person', 'individual'],
            'order': ['order', 'transaction', 'purchase', 'payment', 'sale', 'attempt'],
            
            # Time/Temporal analysis
            'time': ['time', 'date', 'when', 'timestamp', 'created', 'updated', 'period', 'day', 'month'],
            'trend': ['trend', 'over_time', 'temporal', 'timeline', 'history', 'pattern', 'change'],
            
            # Quantity/Metrics
            'count': ['count', 'number', 'amount', 'quantity', 'total', 'sum', 'volume', 'frequency'],
            'rate': ['rate', 'percentage', 'ratio', 'proportion', 'percent', '%'],
            'value': ['value', 'amount', 'price', 'cost', 'fee', 'charge', 'money']
        }
        
        # Dynamic column mapping based on actual data and semantic understanding
        column_mappings = self._build_dynamic_column_mapping(available_columns, semantic_mappings)
        
        # Enhanced intent patterns with business context
        intent_patterns = {
            # Explicit chart type requests (highest priority)
            'pie chart': {'type': 'explicit_chart', 'chart': 'pie', 'focus': 'show proportional distribution as pie chart'},
            'pie': {'type': 'explicit_chart', 'chart': 'pie', 'focus': 'show proportional distribution as pie chart'},
            'íŒŒì´ì°¨íŠ¸': {'type': 'explicit_chart', 'chart': 'pie', 'focus': 'show proportional distribution as pie chart'},
            'íŒŒì´ ì°¨íŠ¸': {'type': 'explicit_chart', 'chart': 'pie', 'focus': 'show proportional distribution as pie chart'},
            'bar chart': {'type': 'explicit_chart', 'chart': 'bar', 'focus': 'show comparative distribution as bar chart'},
            'bar': {'type': 'explicit_chart', 'chart': 'bar', 'focus': 'show comparative distribution as bar chart'},
            'ë§‰ëŒ€ì°¨íŠ¸': {'type': 'explicit_chart', 'chart': 'bar', 'focus': 'show comparative distribution as bar chart'},
            'ë§‰ëŒ€ ì°¨íŠ¸': {'type': 'explicit_chart', 'chart': 'bar', 'focus': 'show comparative distribution as bar chart'},
            'line chart': {'type': 'explicit_chart', 'chart': 'line', 'focus': 'show temporal trends as line chart'},
            'line': {'type': 'explicit_chart', 'chart': 'line', 'focus': 'show temporal trends as line chart'},
            'ì„ ì°¨íŠ¸': {'type': 'explicit_chart', 'chart': 'line', 'focus': 'show temporal trends as line chart'},
            'ì„  ì°¨íŠ¸': {'type': 'explicit_chart', 'chart': 'line', 'focus': 'show temporal trends as line chart'},
            'ì„ ê·¸ëž˜í”„': {'type': 'explicit_chart', 'chart': 'line', 'focus': 'show temporal trends as line chart'},
            'ì„  ê·¸ëž˜í”„': {'type': 'explicit_chart', 'chart': 'line', 'focus': 'show temporal trends as line chart'},
            'ë¼ì¸ì°¨íŠ¸': {'type': 'explicit_chart', 'chart': 'line', 'focus': 'show temporal trends as line chart'},
            'ë¼ì¸ ì°¨íŠ¸': {'type': 'explicit_chart', 'chart': 'line', 'focus': 'show temporal trends as line chart'},
            'scatter plot': {'type': 'explicit_chart', 'chart': 'scatter', 'focus': 'show correlation as scatter plot'},
            'scatterplot': {'type': 'explicit_chart', 'chart': 'scatter', 'focus': 'show correlation as scatter plot'},
            'scatter': {'type': 'explicit_chart', 'chart': 'scatter', 'focus': 'show correlation as scatter plot'},
            'ì‚°í¬ë„': {'type': 'explicit_chart', 'chart': 'scatter', 'focus': 'show correlation as scatter plot'},
            'ì‚°ì ë„': {'type': 'explicit_chart', 'chart': 'scatter', 'focus': 'show correlation as scatter plot'},
            'histogram': {'type': 'explicit_chart', 'chart': 'histogram', 'focus': 'show data distribution as histogram'},
            'ížˆìŠ¤í† ê·¸ëž¨': {'type': 'explicit_chart', 'chart': 'histogram', 'focus': 'show data distribution as histogram'},
            'box plot': {'type': 'explicit_chart', 'chart': 'box', 'focus': 'show statistical distribution as box plot'},
            'boxplot': {'type': 'explicit_chart', 'chart': 'box', 'focus': 'show statistical distribution as box plot'},
            'ë°•ìŠ¤í”Œë¡¯': {'type': 'explicit_chart', 'chart': 'box', 'focus': 'show statistical distribution as box plot'},
            'ìƒìžê·¸ë¦¼': {'type': 'explicit_chart', 'chart': 'box', 'focus': 'show statistical distribution as box plot'},
            'area chart': {'type': 'explicit_chart', 'chart': 'area', 'focus': 'show trend as area chart'},
            'map': {'type': 'explicit_chart', 'chart': 'choropleth', 'focus': 'show geographic distribution on map'},
            'choropleth': {'type': 'explicit_chart', 'chart': 'choropleth', 'focus': 'show geographic distribution as choropleth map'},
            'ì§€ë„': {'type': 'explicit_chart', 'chart': 'choropleth', 'focus': 'show geographic distribution on map'},
            'ì§€ë„ì°¨íŠ¸': {'type': 'explicit_chart', 'chart': 'choropleth', 'focus': 'show geographic distribution as map chart'},
            'ì§€ë„ ì°¨íŠ¸': {'type': 'explicit_chart', 'chart': 'choropleth', 'focus': 'show geographic distribution as map chart'},
            'ë§µì°¨íŠ¸': {'type': 'explicit_chart', 'chart': 'choropleth', 'focus': 'show geographic distribution as map chart'},
            'geo scatter': {'type': 'explicit_chart', 'chart': 'scattergeo', 'focus': 'show geographic points on map'},
            'scattergeo': {'type': 'explicit_chart', 'chart': 'scattergeo', 'focus': 'show geographic points on map'},
            'ì˜ì—­ì°¨íŠ¸': {'type': 'explicit_chart', 'chart': 'area', 'focus': 'show trend as area chart'},
            'heatmap': {'type': 'explicit_chart', 'chart': 'heatmap', 'focus': 'show correlation matrix as heatmap'},
            'ížˆíŠ¸ë§µ': {'type': 'explicit_chart', 'chart': 'heatmap', 'focus': 'show correlation matrix as heatmap'},
            'ì—´ì§€ë„': {'type': 'explicit_chart', 'chart': 'heatmap', 'focus': 'show correlation matrix as heatmap'},
            
            # ADVANCED STATISTICAL CHARTS
            'violin': {'type': 'explicit_chart', 'chart': 'violin', 'focus': 'show distribution as violin plot'},
            'violin plot': {'type': 'explicit_chart', 'chart': 'violin', 'focus': 'show distribution as violin plot'},
            'ë°”ì´ì˜¬ë¦°': {'type': 'explicit_chart', 'chart': 'violin', 'focus': 'show distribution as violin plot'},
            'strip plot': {'type': 'explicit_chart', 'chart': 'strip', 'focus': 'show distribution as strip plot'},
            'ìŠ¤íŠ¸ë¦½': {'type': 'explicit_chart', 'chart': 'strip', 'focus': 'show distribution as strip plot'},
            'density contour': {'type': 'explicit_chart', 'chart': 'density_contour', 'focus': 'show density as contour plot'},
            'ë°€ë„ë“±ê³ ì„ ': {'type': 'explicit_chart', 'chart': 'density_contour', 'focus': 'show density as contour plot'},
            'density heatmap': {'type': 'explicit_chart', 'chart': 'density_heatmap', 'focus': 'show density as heatmap'},
            'ë°€ë„ížˆíŠ¸ë§µ': {'type': 'explicit_chart', 'chart': 'density_heatmap', 'focus': 'show density as heatmap'},
            
            # SPECIALIZED CHARTS
            'funnel': {'type': 'explicit_chart', 'chart': 'funnel', 'focus': 'show conversion as funnel chart'},
            'funnel chart': {'type': 'explicit_chart', 'chart': 'funnel', 'focus': 'show conversion as funnel chart'},
            'ê¹”ë•Œê¸°': {'type': 'explicit_chart', 'chart': 'funnel', 'focus': 'show conversion as funnel chart'},
            'waterfall': {'type': 'explicit_chart', 'chart': 'waterfall', 'focus': 'show cumulative effect as waterfall'},
            'í­í¬ì°¨íŠ¸': {'type': 'explicit_chart', 'chart': 'waterfall', 'focus': 'show cumulative effect as waterfall'},
            'treemap': {'type': 'explicit_chart', 'chart': 'treemap', 'focus': 'show hierarchy as treemap'},
            'íŠ¸ë¦¬ë§µ': {'type': 'explicit_chart', 'chart': 'treemap', 'focus': 'show hierarchy as treemap'},
            'sunburst': {'type': 'explicit_chart', 'chart': 'sunburst', 'focus': 'show hierarchy as sunburst'},
            'ì„ ë²„ìŠ¤íŠ¸': {'type': 'explicit_chart', 'chart': 'sunburst', 'focus': 'show hierarchy as sunburst'},
            
            # 3D CHARTS
            '3d scatter': {'type': 'explicit_chart', 'chart': 'scatter_3d', 'focus': 'show 3D scatter plot'},
            'scatter 3d': {'type': 'explicit_chart', 'chart': 'scatter_3d', 'focus': 'show 3D scatter plot'},
            '3d ì‚°ì ë„': {'type': 'explicit_chart', 'chart': 'scatter_3d', 'focus': 'show 3D scatter plot'},
            'surface': {'type': 'explicit_chart', 'chart': 'surface', 'focus': 'show 3D surface plot'},
            'í‘œë©´ì°¨íŠ¸': {'type': 'explicit_chart', 'chart': 'surface', 'focus': 'show 3D surface plot'},
            '3d surface': {'type': 'explicit_chart', 'chart': 'surface', 'focus': 'show 3D surface plot'},
            
            # FINANCIAL CHARTS  
            'candlestick': {'type': 'explicit_chart', 'chart': 'candlestick', 'focus': 'show financial data as candlestick'},
            'ìº”ë“¤ìŠ¤í‹±': {'type': 'explicit_chart', 'chart': 'candlestick', 'focus': 'show financial data as candlestick'},
            'ohlc': {'type': 'explicit_chart', 'chart': 'ohlc', 'focus': 'show OHLC financial data'},
            
            # MULTIVARIATE CHARTS
            'parallel coordinates': {'type': 'explicit_chart', 'chart': 'parallel_coordinates', 'focus': 'show multivariate as parallel coordinates'},
            'í‰í–‰ì¢Œí‘œ': {'type': 'explicit_chart', 'chart': 'parallel_coordinates', 'focus': 'show multivariate as parallel coordinates'},
            'parallel categories': {'type': 'explicit_chart', 'chart': 'parallel_categories', 'focus': 'show categories as parallel plot'},
            'í‰í–‰ì¹´í…Œê³ ë¦¬': {'type': 'explicit_chart', 'chart': 'parallel_categories', 'focus': 'show categories as parallel plot'},
            
            # SPECIALIZED VISUALIZATION
            'radar': {'type': 'explicit_chart', 'chart': 'radar', 'focus': 'show multivariate as radar chart'},
            'radar chart': {'type': 'explicit_chart', 'chart': 'radar', 'focus': 'show multivariate as radar chart'},
            'ë ˆì´ë”': {'type': 'explicit_chart', 'chart': 'radar', 'focus': 'show multivariate as radar chart'},
            'spider': {'type': 'explicit_chart', 'chart': 'radar', 'focus': 'show multivariate as spider chart'},
            
            # DISTRIBUTION CHARTS
            'distplot': {'type': 'explicit_chart', 'chart': 'distplot', 'focus': 'show distribution with KDE'},
            'ë¶„í¬í”Œë¡¯': {'type': 'explicit_chart', 'chart': 'distplot', 'focus': 'show distribution with KDE'},
            'ecdf': {'type': 'explicit_chart', 'chart': 'ecdf', 'focus': 'show empirical cumulative distribution'},
            'ëˆ„ì ë¶„í¬': {'type': 'explicit_chart', 'chart': 'ecdf', 'focus': 'show empirical cumulative distribution'},
            
            # Distribution/Grouping patterns
            'sort by': {'type': 'distribution', 'chart': 'bar', 'focus': 'show ranked distribution'},
            'group by': {'type': 'distribution', 'chart': 'bar', 'focus': 'show categorical grouping'},
            'breakdown': {'type': 'distribution', 'chart': 'pie', 'focus': 'show proportional breakdown'},
            'distribution': {'type': 'distribution', 'chart': 'pie', 'focus': 'show data distribution'},
            'proportion': {'type': 'distribution', 'chart': 'pie', 'focus': 'show proportional distribution'},
            'percentage': {'type': 'distribution', 'chart': 'pie', 'focus': 'show percentage distribution'},
            'share': {'type': 'distribution', 'chart': 'pie', 'focus': 'show share distribution'},
            'analyze': {'type': 'distribution', 'chart': 'bar', 'focus': 'perform comprehensive analysis'},
            
            # Comparison patterns
            'compare': {'type': 'comparison', 'chart': 'bar', 'focus': 'compare across categories'},
            'vs': {'type': 'comparison', 'chart': 'bar', 'focus': 'compare alternatives'},
            'versus': {'type': 'comparison', 'chart': 'bar', 'focus': 'compare alternatives'},
            'difference': {'type': 'comparison', 'chart': 'bar', 'focus': 'show differences'},
            
            # Temporal/Trend patterns
            'trend': {'type': 'temporal', 'chart': 'line', 'focus': 'show trends over time'},
            'over time': {'type': 'temporal', 'chart': 'line', 'focus': 'show temporal patterns'},
            'timeline': {'type': 'temporal', 'chart': 'line', 'focus': 'show timeline progression'},
            'history': {'type': 'temporal', 'chart': 'line', 'focus': 'show historical data'},
            'growth': {'type': 'temporal', 'chart': 'line', 'focus': 'show growth patterns'},
            'change': {'type': 'temporal', 'chart': 'line', 'focus': 'show changes over time'},
            
            # Ranking patterns
            'top': {'type': 'ranking', 'chart': 'bar', 'focus': 'show highest values'},
            'bottom': {'type': 'ranking', 'chart': 'bar', 'focus': 'show lowest values'},
            'best': {'type': 'ranking', 'chart': 'bar', 'focus': 'show best performing'},
            'worst': {'type': 'ranking', 'chart': 'bar', 'focus': 'show worst performing'},
            'most': {'type': 'ranking', 'chart': 'bar', 'focus': 'show most frequent/common'},
            'least': {'type': 'ranking', 'chart': 'bar', 'focus': 'show least frequent'},
            'highest': {'type': 'ranking', 'chart': 'bar', 'focus': 'show highest values'},
            'lowest': {'type': 'ranking', 'chart': 'bar', 'focus': 'show lowest values'},
            
            # Statistical patterns
            'correlation': {'type': 'correlation', 'chart': 'scatter', 'focus': 'show statistical relationships'},
            'relationship': {'type': 'correlation', 'chart': 'scatter', 'focus': 'analyze relationships'},
            'impact': {'type': 'correlation', 'chart': 'scatter', 'focus': 'show impact analysis'},
            
            # Aggregation patterns
            'count': {'type': 'distribution', 'chart': 'bar', 'focus': 'show item counts'},
            'sum': {'type': 'distribution', 'chart': 'bar', 'focus': 'show sum aggregation'},
            'average': {'type': 'distribution', 'chart': 'bar', 'focus': 'show average values'},
            'total': {'type': 'distribution', 'chart': 'bar', 'focus': 'show total values'}
        }
        
        # Advanced pattern matching with context scoring
        detected_intent = self._detect_intent_with_context(question_lower, intent_patterns)
        
        # Contextual refinement based on data characteristics
        detected_intent = self._refine_intent_with_data_context(detected_intent, available_columns, question_lower)
        
        # Intelligent column mapping with semantic scoring
        suggested_columns = self._map_question_to_columns(question_lower, available_columns, column_mappings)
        print(f"ðŸ” _map_question_to_columns result: {suggested_columns}")
        
        # Final validation and optimization
        final_analysis = self._optimize_analysis_strategy(suggested_columns, detected_intent, question_lower)
        
        print(f"ðŸŽ¯ Intent Analysis Results:")
        print(f"   Chart Type: {final_analysis['chart_type']}")
        print(f"   Intent Type: {final_analysis['intent_type']}")
        print(f"   Confidence: {final_analysis['confidence']}")
        print(f"   X Column: {final_analysis['columns']['x']}")
        print(f"   Y Column: {final_analysis['columns']['y']}")
        print(f"   Reasoning: {final_analysis['reasoning']}")

        return {
            'suggested_columns': final_analysis['columns'],
            'analysis_focus': final_analysis['focus'],
            'chart_type': final_analysis['chart_type'],
            'intent_type': final_analysis['intent_type'],
            'confidence': final_analysis['confidence'],
            'reasoning': final_analysis['reasoning']
        }
    
    def _extract_json_from_response(self, content: str) -> Dict[str, Any]:
        """Extract JSON from mixed content response"""
        
        # Try to find JSON block in the content
        json_match = re.search(r'```json\s*(\{.*?\})\s*```', content, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group(1))
            except json.JSONDecodeError:
                pass
        
        # Try to find JSON without markdown formatting
        json_match = re.search(r'(\{[^{}]*"insights"[^{}]*\})', content, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group(1))
            except json.JSONDecodeError:
                pass
        
        return None
    
    def _create_conversational_analysis(self, df: pd.DataFrame, question: str, content: str) -> Dict[str, Any]:
        """Create conversational analysis from raw AI response"""
        # Use intent analysis for smart column selection
        intent_analysis = self._analyze_user_intent(question, df.columns.tolist())
        print(f"ðŸ”„ _create_conversational_analysis intent_analysis:")
        print(f"   Chart Type: {intent_analysis.get('chart_type', 'N/A')}")
        print(f"   Suggested Columns: {intent_analysis.get('suggested_columns', {})}")
        x_col = intent_analysis['suggested_columns']['x']
        y_col = intent_analysis['suggested_columns']['y']
        
        # Clean up the content - remove JSON markers and format conversationally
        cleaned_content = content
        
        # Remove JSON code block markers
        cleaned_content = re.sub(r'```json\s*', '', cleaned_content)
        cleaned_content = re.sub(r'\s*```', '', cleaned_content)
        
        # Remove raw JSON structure if present
        cleaned_content = re.sub(r'\{\s*"insights":\s*\[', '', cleaned_content)
        cleaned_content = re.sub(r'\]\s*,?\s*"chart_type".*?\}', '', cleaned_content, flags=re.DOTALL)
        
        # Extract insights from quotes
        insight_pattern = r'"([^"]{30,})"'
        insights_found = re.findall(insight_pattern, cleaned_content)
        
        # Clean up insights and make them conversational
        conversational_insights = []
        for insight in insights_found[:6]:
            # Remove escape characters and extra whitespace
            clean_insight = insight.replace('\\"', '"').replace('\\n', ' ').strip()
            if len(clean_insight) > 20:
                conversational_insights.append(clean_insight)
        
        # If no good insights found, create meaningful ones from the content
        if not conversational_insights:
            # Try to extract meaningful sentences from the raw content
            sentences = [s.strip() for s in re.split(r'[.!?]+', cleaned_content) if len(s.strip()) > 30]
            conversational_insights = sentences[:5]
        
        # Ensure we have at least some basic insights
        if not conversational_insights:
            conversational_insights = [
                f"ë°ì´í„°ì…‹ì—ì„œ {len(df):,}ê°œì˜ ë ˆì½”ë“œì™€ {len(df.columns)}ê°œì˜ ì»¬ëŸ¼ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤.",
                f"ì£¼ìš” ì»¬ëŸ¼ìœ¼ë¡œëŠ” {', '.join(df.columns[:3])} ë“±ì´ ìžˆìŠµë‹ˆë‹¤.",
                "ë°ì´í„°ì˜ íŒ¨í„´ê³¼ íŠ¹ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤."
            ]
        
        # Create a conversational summary
        summary = f"'{question}' ì§ˆë¬¸ì— ëŒ€í•œ ë¶„ì„ ê²°ê³¼ìž…ë‹ˆë‹¤. "
        if conversational_insights:
            # Use the first insight as part of summary
            first_insight = conversational_insights[0]
            if len(first_insight) > 100:
                first_insight = first_insight[:100] + "..."
            summary += first_insight
        
        # Generate contextual follow-up questions
        follow_up_questions = self._generate_follow_up_questions(question, df.columns.tolist(), intent_analysis)
        
        return convert_numpy_types({
            "insights": conversational_insights,
            "chart_type": intent_analysis['chart_type'],
            "chart_columns": intent_analysis['suggested_columns'],  # Use smart analysis columns
            "summary": summary,
            "follow_up_questions": follow_up_questions
        })
    
    def _create_fallback_analysis(self, df: pd.DataFrame, question: str, content: str) -> Dict[str, Any]:
        """Create fallback analysis when JSON parsing fails - deprecated, use _create_conversational_analysis"""
        return self._create_conversational_analysis(df, question, content)
    
    def _convert_comprehensive_analysis(self, comprehensive_result: Dict[str, Any]) -> Dict[str, Any]:
        """Convert comprehensive analysis format to backward compatible format"""
        # Extract insights from various sections
        all_insights = []
        
        # Add data quality insights
        if "data_quality" in comprehensive_result:
            dq = comprehensive_result["data_quality"]
            if "assessment" in dq:
                all_insights.append(f"Data Quality: {dq['assessment']}")
            if "completeness_score" in dq:
                all_insights.append(f"Data completeness score: {dq['completeness_score']}%")
        
        # Add main insights
        if "insights" in comprehensive_result:
            all_insights.extend(comprehensive_result["insights"][:4])
        
        # Add business implications
        if "business_implications" in comprehensive_result:
            bi = comprehensive_result["business_implications"]
            if "opportunities" in bi and bi["opportunities"]:
                all_insights.append(f"Key Opportunity: {bi['opportunities'][0]}")
            if "recommendations" in bi and bi["recommendations"]:
                all_insights.append(f"Recommendation: {bi['recommendations'][0]}")
        
        # Create backward compatible structure
        return {
            "insights": all_insights[:8],  # Limit to reasonable number
            "chart_type": comprehensive_result.get("chart_type", "bar"),
            "chart_columns": comprehensive_result.get("chart_columns", {}),
            "summary": comprehensive_result.get("summary", "Comprehensive data analysis completed"),
            "comprehensive_data": comprehensive_result  # Keep full data for future use
        }
    
    def _build_dynamic_column_mapping(self, available_columns: List[str], semantic_mappings: Dict[str, List[str]]) -> Dict[str, List[str]]:
        """Build intelligent column mappings based on actual data and semantic understanding"""
        mappings = {}
        
        for col in available_columns:
            col_lower = col.lower().replace('_', ' ').replace('-', ' ')
            words = col_lower.split()
            
            # Create mappings for individual words and combinations
            for i, word1 in enumerate(words):
                if len(word1) > 2:
                    mappings[word1] = mappings.get(word1, []) + [col]
                    
                    # Create two-word combinations
                    for j in range(i+1, len(words)):
                        word2 = words[j]
                        if len(word2) > 2:
                            composite = f"{word1} {word2}"
                            mappings[composite] = mappings.get(composite, []) + [col]
        
        return mappings
    
    def _detect_intent_with_context(self, question_lower: str, intent_patterns: Dict) -> Dict[str, str]:
        """Advanced pattern matching with context scoring - prioritizes explicit chart requests"""
        pattern_scores = {}
        
        for pattern, intent in intent_patterns.items():
            score = 0
            
            if pattern in question_lower:
                # Give much higher priority to explicit chart type requests
                if intent['type'] == 'explicit_chart':
                    score += 50  # Very high priority for explicit chart requests
                else:
                    score += 10
                
                position = question_lower.find(pattern)
                score += max(0, 5 - position // 10)
            
            # Check for partial matches
            pattern_words = pattern.split()
            if all(word in question_lower for word in pattern_words):
                if intent['type'] == 'explicit_chart':
                    score += 25  # High priority even for partial matches
                else:
                    score += 5
            
            # Special handling for common chart type keywords
            if intent['type'] == 'explicit_chart':
                chart_type = intent['chart']
                if chart_type in question_lower:
                    score += 30
            
            if score > 0:
                pattern_scores[pattern] = score
        
        if pattern_scores:
            best_pattern = max(pattern_scores, key=pattern_scores.get)
            return intent_patterns[best_pattern]
        
        # Smart default based on question characteristics
        if any(word in question_lower for word in ['proportion', 'percentage', 'share', 'distribution', 'breakdown']):
            return {'type': 'distribution', 'chart': 'pie', 'focus': 'show proportional distribution'}
        elif any(word in question_lower for word in ['trend', 'over time', 'growth', 'change']):
            return {'type': 'temporal', 'chart': 'line', 'focus': 'show trends over time'}
        elif any(word in question_lower for word in ['correlation', 'relationship', 'vs', 'versus']):
            return {'type': 'correlation', 'chart': 'scatter', 'focus': 'show relationships'}
        
        return {'type': 'distribution', 'chart': 'bar', 'focus': 'analyze data distribution'}
    
    def _refine_intent_with_data_context(self, intent: Dict, available_columns: List[str], question: str) -> Dict:
        """Refine intent based on data characteristics"""
        # Enhance focus based on domain context
        if any(term in question.lower() for term in ['failure', 'error', 'problem']):
            intent['focus'] = intent['focus'].replace('show', 'analyze failure patterns in')
            
        if any(term in question.lower() for term in ['success', 'complete', 'approved']):
            intent['focus'] = intent['focus'].replace('show', 'analyze success patterns in')
        
        return intent
    
    def _map_question_to_columns(self, question: str, available_columns: List[str], column_mappings: Dict) -> Dict[str, str]:
        """Intelligent semantic mapping of questions to columns"""
        column_scores = {}
        
        # Check chart type requests
        is_scatter_request = any(term in question.lower() for term in ['scatter', 'ì‚°í¬ë„', 'ì‚°ì ë„', 'correlation', 'ìƒê´€ê´€ê³„'])
        is_histogram_request = any(term in question.lower() for term in ['histogram', 'ížˆìŠ¤í† ê·¸ëž¨', 'distribution', 'ë¶„í¬'])
        is_heatmap_request = any(term in question.lower() for term in ['heatmap', 'ížˆíŠ¸ë§µ', 'ì—´ì§€ë„', 'correlation matrix', 'ìƒê´€ê´€ê³„'])
        is_recommendation_request = any(term in question.lower() for term in [
            'ì¶”ì²œ', 'recommend', 'ì œì•ˆ', 'suggest', 'ì–´ë–¤ ì°¨íŠ¸', 'what chart', 'ë¬´ìŠ¨ ì°¨íŠ¸', 'ì¢‹ì€ ì°¨íŠ¸', 
            'ì í•©í•œ ì°¨íŠ¸', 'ìµœì ', 'optimal', 'best', 'ë¶„ì„ ë°©ë²•', 'analysis method'
        ])
        
        for col in available_columns:
            score = 0
            col_lower = col.lower()
            
            # Direct word matches
            question_words = question.split()
            col_words = col_lower.replace('_', ' ').replace('-', ' ').split()
            
            # Score based on word matches
            for qw in question_words:
                for cw in col_words:
                    if qw == cw:
                        score += 10
                    elif qw in cw or cw in qw:
                        score += 5
            
            # Business logic scoring
            if 'failure' in question and 'message' in col_lower:
                score += 15
            if 'card' in question and 'network' in col_lower:
                score += 12
            if 'country' in question and 'country' in col_lower:
                score += 12
            if 'reason' in question and 'message' in col_lower:
                score += 10
                
            # Penalize ID columns unless specifically asked
            if 'id' in col_lower and 'id' not in question:
                score -= 5
            
            column_scores[col] = score
        
        # Special handling for different chart types
        if is_scatter_request:
            # Enhanced logic for correlation analysis - can be categorical vs numeric
            correlation_keywords = ['ìƒê´€ê´€ê³„', 'ìƒê´€ì„±', 'correlation', 'ê´€ê³„', 'ì—°ê´€', 'vs', 'versus', 'ê°„ì˜']
            is_correlation_analysis = any(keyword in question for keyword in correlation_keywords)

            if is_correlation_analysis:
                # For correlation, identify categorical and numeric columns from the question
                age_keywords = ['ë‚˜ì´', 'age', 'ì—°ë ¹', 'class']
                weight_keywords = ['ë¬´ê²Œ', 'weight', 'ì²´ì¤‘', 'kg']

                categorical_col = None
                numeric_col = None

                # Find Age Class column
                for col in available_columns:
                    if any(age_kw in col.lower() for age_kw in age_keywords):
                        categorical_col = col
                        break

                # Find Weight column
                for col in available_columns:
                    if any(weight_kw in col.lower() for weight_kw in weight_keywords):
                        numeric_col = col
                        break

                # Use the identified columns if found
                if categorical_col and numeric_col:
                    result = {'x': categorical_col, 'y': numeric_col}
                    print(f"ðŸŽ¯ Found correlation pair: {categorical_col} vs {numeric_col}")
                    print(f"ðŸŽ¯ Returning correlation result: {result}")
                    return result

                # Fallback: try to match any mentioned columns in the question
                question_words = question.lower().split()
                mentioned_cols = []
                for col in available_columns:
                    col_words = col.lower().replace('_', ' ').replace('-', ' ').split()
                    if any(qw in col_words or any(qw in cw or cw in qw for cw in col_words) for qw in question_words):
                        mentioned_cols.append(col)

                if len(mentioned_cols) >= 2:
                    # Prioritize categorical vs numeric combination
                    numeric_columns = self._identify_numeric_columns(mentioned_cols)
                    categorical_columns = [col for col in mentioned_cols if col not in numeric_columns]

                    if categorical_columns and numeric_columns:
                        print(f"ðŸŽ¯ Using categorical vs numeric: {categorical_columns[0]} vs {numeric_columns[0]}")
                        return {'x': categorical_columns[0], 'y': numeric_columns[0]}
                    else:
                        print(f"ðŸŽ¯ Using first two mentioned columns: {mentioned_cols[0]} vs {mentioned_cols[1]}")
                        return {'x': mentioned_cols[0], 'y': mentioned_cols[1]}

            # Original scatter plot logic for non-correlation requests
            numeric_columns = self._identify_numeric_columns(available_columns)
            if len(numeric_columns) >= 2:
                # Use the two best numeric columns for scatter plot
                x_col = numeric_columns[0]
                y_col = numeric_columns[1]

                # If question mentions specific columns, try to use those
                for col in numeric_columns:
                    if any(word in col.lower() for word in question.lower().split()):
                        if x_col == numeric_columns[0]:  # First match becomes x
                            x_col = col
                        else:  # Second match becomes y
                            y_col = col
                            break

                return {'x': x_col, 'y': y_col}
            else:
                # Fallback: use best scored column with a numeric column if available
                best_col = max(column_scores, key=column_scores.get) if column_scores else available_columns[0]
                if numeric_columns:
                    return {'x': best_col, 'y': numeric_columns[0]}
                return {'x': best_col, 'y': 'Count'}
        
        elif is_histogram_request:
            # Histogram only needs one numeric column
            numeric_columns = self._identify_numeric_columns(available_columns)
            if numeric_columns:
                # Find the best numeric column based on question
                best_numeric = numeric_columns[0]
                for col in numeric_columns:
                    if any(word in col.lower() for word in question.lower().split()):
                        best_numeric = col
                        break
                return {'x': best_numeric, 'y': 'Count'}
            else:
                # No numeric columns available
                best_col = max(column_scores, key=column_scores.get) if column_scores else available_columns[0]
                return {'x': best_col, 'y': 'Count'}
        
        elif is_heatmap_request:
            # Heatmap doesn't need specific x,y columns - uses all numeric columns
            numeric_columns = self._identify_numeric_columns(available_columns)
            if len(numeric_columns) >= 2:
                return {'x': 'all_numeric', 'y': 'all_numeric'}
            else:
                return {'x': 'insufficient_numeric', 'y': 'insufficient_numeric'}
        
        elif is_recommendation_request:
            # Return special marker for recommendation requests
            return {'x': 'recommendation_request', 'y': 'recommendation_request'}
        
        # Select best column for non-scatter charts
        if column_scores and max(column_scores.values()) > 0:
            best_col = max(column_scores, key=column_scores.get)
            return {'x': best_col, 'y': 'Count'}
        
        # Fallback logic
        priority_indicators = ['message', 'status', 'type', 'network', 'country', 'reason']
        for indicator in priority_indicators:
            for col in available_columns:
                if indicator in col.lower() and 'id' not in col.lower():
                    return {'x': col, 'y': 'Count'}
        
        # Final fallback
        non_id_cols = [col for col in available_columns if 'id' not in col.lower()]
        if non_id_cols:
            return {'x': non_id_cols[0], 'y': 'Count'}
        
        return {'x': available_columns[0] if available_columns else 'unknown', 'y': 'Count'}
    
    def _identify_numeric_columns(self, available_columns: List[str]) -> List[str]:
        """Identify potentially numeric columns based on column names"""
        numeric_indicators = [
            'amount', 'value', 'price', 'cost', 'fee', 'rate', 'count', 'number', 'num', 
            'quantity', 'size', 'length', 'width', 'height', 'weight', 'age', 'score',
            'total', 'sum', 'avg', 'mean', 'max', 'min', 'percentage', 'percent', '%'
        ]
        
        numeric_columns = []
        for col in available_columns:
            col_lower = col.lower()
            # Skip ID columns
            if 'id' in col_lower:
                continue
            # Check for numeric indicators
            if any(indicator in col_lower for indicator in numeric_indicators):
                numeric_columns.append(col)
            # Check if column name suggests it's numeric (ends with numbers, etc.)
            elif any(char.isdigit() for char in col_lower):
                numeric_columns.append(col)
        
        return numeric_columns
    
    def _optimize_analysis_strategy(self, columns: Dict, intent: Dict, question: str) -> Dict:
        """Final optimization of analysis strategy with enhanced categorical-numeric handling"""
        print(f"ðŸ”§ _optimize_analysis_strategy input - columns: {columns}")
        confidence = 0.5
        reasoning = ["Basic analysis strategy"]

        if columns['x'] != 'unknown':
            confidence += 0.2
            reasoning.append(f"Found relevant column: {columns['x']}")

        # Boost confidence for clear intents
        clear_intents = ['sort by', 'group by', 'breakdown', 'compare', 'trend']
        if any(intent_word in question for intent_word in clear_intents):
            confidence += 0.3
            reasoning.append("Clear user intent detected")

        # Enhanced correlation/relationship analysis for categorical-numeric data
        correlation_keywords = ['ìƒê´€ê´€ê³„', 'ìƒê´€ì„±', 'correlation', 'relationship', 'ê´€ê³„', 'ì—°ê´€', 'vs', 'versus', 'ê°„ì˜']
        if any(keyword in question.lower() for keyword in correlation_keywords):
            confidence += 0.4
            reasoning.append("Correlation analysis detected")

        # Use original intent chart type (don't force box plot)
        chart_type = intent['chart']

        # Domain-specific optimizations
        if 'failure' in question.lower() and 'message' in columns['x'].lower():
            confidence = 0.9
            reasoning.append("Perfect match: failure analysis with message column")

        # Age class analysis optimization - keep original chart type but boost confidence
        age_keywords = ['ë‚˜ì´', 'age', 'ì—°ë ¹', 'class']
        weight_keywords = ['ë¬´ê²Œ', 'weight', 'ì²´ì¤‘', 'kg']
        if (any(age_kw in question.lower() for age_kw in age_keywords) and
            any(weight_kw in question.lower() for weight_kw in weight_keywords)):
            confidence = 0.95
            reasoning.append("Age-weight analysis detected: high confidence")

        # Distribution optimization (but preserve special chart types like box)
        if intent['type'] == 'distribution' and chart_type not in ['bar', 'pie', 'box', 'violin', 'histogram']:
            chart_type = 'bar'
            reasoning.append("Optimized chart type for distribution")

        result = {
            'columns': columns,
            'focus': f"{intent['focus']} of {columns['x']}" + (f" vs {columns['y']}" if columns['y'] != 'unknown' else ""),
            'chart_type': chart_type,
            'intent_type': intent['type'],
            'confidence': min(1.0, confidence),
            'reasoning': reasoning
        }
        print(f"ðŸ”§ _optimize_analysis_strategy result - columns: {result['columns']}")
        return result
    
    async def answer_general_question(self, question: str) -> str:
        """íŒŒì¼ ì—†ì´ ì¼ë°˜ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤. ì‹œë‚˜ë¦¬ì˜¤, ê°€ì •, í‘œ ìƒì„± ë“±ì„ í¬í•¨í•œ í¬ê´„ì ì¸ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤."""
        try:
            # Check if API key is properly set
            if not settings.OPENAI_API_KEY or settings.OPENAI_API_KEY == "your_openai_api_key_here":
                return "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”."

            # ì§ˆë¬¸ ìœ í˜• ë¶„ì„ ë° ë§žì¶¤í˜• í”„ë¡¬í”„íŠ¸ ìƒì„±
            enhanced_prompt = self._create_enhanced_general_prompt(question)

            response = await self.client.chat.completions.create(
                model="gpt-4o",  # ë” ê°•ë ¥í•œ ëª¨ë¸ ì‚¬ìš©
                messages=[
                    {
                        "role": "system",
                        "content": enhanced_prompt
                    },
                    {"role": "user", "content": question}
                ],
                temperature=0.7,
                max_tokens=2000  # ë” ê¸´ ì‘ë‹µ í—ˆìš©
            )

            return response.choices[0].message.content.strip()

        except Exception as e:
            print(f"General question AI error: {str(e)}")
            return f"ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    def _create_enhanced_general_prompt(self, question: str) -> str:
        """ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ ë§žì¶¤í˜• ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        question_lower = question.lower()

        # ì‹œë‚˜ë¦¬ì˜¤/ê°€ì • ê¸°ë°˜ ì§ˆë¬¸ ê°ì§€
        scenario_keywords = ['ê°€ì •', 'ì‹œë‚˜ë¦¬ì˜¤', 'ë§Œì•½', 'if', 'suppose', 'ìƒí™©', 'ê²½ìš°', 'ì˜ˆë¥¼ ë“¤ì–´', 'ê°€ë ¹']
        table_keywords = ['í‘œ', 'table', 'í…Œì´ë¸”', 'ë°ì´í„°', 'ëª©ë¡', 'list', 'ë¹„êµ', 'ì •ë¦¬', 'ìš”ì•½']
        logic_keywords = ['ë¡œì§', 'logic', 'ë°©ë²•', 'ì ˆì°¨', 'ë‹¨ê³„', 'step', 'í”„ë¡œì„¸ìŠ¤', 'process', 'ì•Œê³ ë¦¬ì¦˜']
        analysis_keywords = ['ë¶„ì„', 'analysis', 'í•´ì„', 'í‰ê°€', 'ê²€í† ', 'ì¡°ì‚¬', 'ì—°êµ¬']
        calculation_keywords = ['ê³„ì‚°', 'ìˆ˜ì¹˜', 'í†µê³„', 'ì˜ˆìƒ', 'ì¶”ì •', 'ì¸¡ì •']

        is_scenario = any(keyword in question_lower for keyword in scenario_keywords)
        is_table = any(keyword in question_lower for keyword in table_keywords)
        is_logic = any(keyword in question_lower for keyword in logic_keywords)
        is_analysis = any(keyword in question_lower for keyword in analysis_keywords)
        is_calculation = any(keyword in question_lower for keyword in calculation_keywords)

        base_prompt = """ë‹¹ì‹ ì€ ì „ë¬¸ì ì´ê³  ë„ì›€ì´ ë˜ëŠ” AI ë°ì´í„° ë¶„ì„ ì–´ì‹œìŠ¤í„´íŠ¸ìž…ë‹ˆë‹¤.
ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”."""

        if is_scenario:
            return base_prompt + """

**ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„ ì „ë¬¸ê°€ë¡œì„œ:**
- ì œì‹œëœ ê°€ì •ì´ë‚˜ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ë…¼ë¦¬ì ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”
- ê°€ëŠ¥í•œ ê²°ê³¼ì™€ ì˜í–¥ì„ ì²´ê³„ì ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”
- ì‹¤ì œ ë°ì´í„°ê°€ ìžˆë‹¤ë©´ ì–´ë–»ê²Œ ë¶„ì„í• ì§€ êµ¬ì²´ì ì¸ ë°©ë²•ì„ ì œì‹œí•˜ì„¸ìš”
- ì—¬ëŸ¬ ê´€ì ì—ì„œ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ê²€í† í•˜ê³  ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ì„¸ìš”
- í•„ìš”ì‹œ ì˜ˆì‹œ ë°ì´í„°ë‚˜ ìƒ˜í”Œ í…Œì´ë¸”ì„ í¬í•¨í•˜ì„¸ìš”
- ìˆ˜í•™ì  ê³„ì‚°ì´ë‚˜ ê³µì‹ì´ í•„ìš”í•œ ê²½ìš° LaTeX ìˆ˜ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš” (ì˜ˆ: $\\frac{ë³€í™”ëŸ‰}{ê¸°ì¤€ê°’} \\times 100$)

ë‹µë³€ í˜•ì‹:
1. ì‹œë‚˜ë¦¬ì˜¤ ìš”ì•½
2. ì£¼ìš” ê°€ì •ì‚¬í•­
3. ì˜ˆìƒ ê²°ê³¼ ë° ì˜í–¥
4. ë¶„ì„ ë°©ë²•ë¡ 
5. ì‹¤í–‰ ê°€ëŠ¥í•œ ì œì•ˆì‚¬í•­"""

        elif is_table:
            return base_prompt + """

**í‘œ/ë°ì´í„° êµ¬ì¡° ì „ë¬¸ê°€ë¡œì„œ:**
- ìš”ì²­ëœ ì •ë³´ë¥¼ ì²´ê³„ì ì¸ í‘œ í˜•íƒœë¡œ ì •ë¦¬í•˜ì„¸ìš”
- ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”
- ì»¬ëŸ¼ê³¼ í–‰ì„ ë…¼ë¦¬ì ìœ¼ë¡œ êµ¬ì„±í•˜ì„¸ìš”
- í‘œì— ëŒ€í•œ ì„¤ëª…ê³¼ ì¸ì‚¬ì´íŠ¸ë¥¼ í•¨ê»˜ ì œê³µí•˜ì„¸ìš”
- ì‹¤ì œ ë°ì´í„° ë¶„ì„ ì‹œ ê³ ë ¤ì‚¬í•­ì„ í¬í•¨í•˜ì„¸ìš”

í‘œ í˜•ì‹ ì˜ˆì‹œ:
| í•­ëª© | ê°’1 | ê°’2 | ì„¤ëª… |
|------|-----|-----|------|
| ... | ... | ... | ... |

í‘œ í›„ì—ëŠ” ì£¼ìš” íŒ¨í„´ì´ë‚˜ ì¸ì‚¬ì´íŠ¸ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”."""

        elif is_logic:
            return base_prompt + """

**ë¡œì§/í”„ë¡œì„¸ìŠ¤ ì„¤ê³„ ì „ë¬¸ê°€ë¡œì„œ:**
- ë‹¨ê³„ë³„ë¡œ ëª…í™•í•œ ì ˆì°¨ë¥¼ ì œì‹œí•˜ì„¸ìš”
- ê° ë‹¨ê³„ì˜ ëª©ì ê³¼ ë°©ë²•ì„ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”
- ì‹¤ì œ êµ¬í˜„ ì‹œ ê³ ë ¤ì‚¬í•­ì„ í¬í•¨í•˜ì„¸ìš”
- ê°€ëŠ¥í•œ ë¬¸ì œì ê³¼ í•´ê²°ë°©ì•ˆì„ ì œì‹œí•˜ì„¸ìš”
- ë°ì´í„° ë¶„ì„ ê´€ì ì—ì„œì˜ ì ‘ê·¼ë²•ì„ í¬í•¨í•˜ì„¸ìš”

ë‹µë³€ í˜•ì‹:
1. ê°œìš” ë° ëª©í‘œ
2. ë‹¨ê³„ë³„ ìƒì„¸ ì ˆì°¨
3. ê° ë‹¨ê³„ë³„ ê³ ë ¤ì‚¬í•­
4. ì˜ˆìƒ ê²°ê³¼ë¬¼
5. í’ˆì§ˆ ê²€ì¦ ë°©ë²•"""

        elif is_analysis:
            return base_prompt + """

**ë¶„ì„ ì „ë¬¸ê°€ë¡œì„œ:**
- ì£¼ì œì— ëŒ€í•œ ë‹¤ê°ë„ ë¶„ì„ì„ ì œê³µí•˜ì„¸ìš”
- ì •ëŸ‰ì /ì •ì„±ì  ê´€ì ì„ ëª¨ë‘ ê³ ë ¤í•˜ì„¸ìš”
- ì‹¤ì œ ë°ì´í„°ê°€ ìžˆë‹¤ë©´ ì–´ë–¤ ì°¨íŠ¸ë‚˜ ì§€í‘œê°€ ìœ ìš©í• ì§€ ì œì•ˆí•˜ì„¸ìš”
- ë¹„ì¦ˆë‹ˆìŠ¤ ë˜ëŠ” ì‹¤ë¬´ì  ê´€ì ì—ì„œì˜ ì‹œì‚¬ì ì„ ë„ì¶œí•˜ì„¸ìš”
- ì¶”ê°€ ë¶„ì„ì´ í•„ìš”í•œ ì˜ì—­ì„ ì‹ë³„í•˜ì„¸ìš”

ë‹µë³€ í˜•ì‹:
1. í˜„í™© ë¶„ì„
2. í•µì‹¬ ìš”ì¸ ì‹ë³„
3. íŒ¨í„´ ë° íŠ¸ë Œë“œ
4. ìœ„í—˜ìš”ì†Œ ë° ê¸°íšŒ
5. ê°œì„  ë°©í–¥ ì œì•ˆ"""

        elif is_calculation:
            return base_prompt + """

**ìˆ˜ì¹˜ ë¶„ì„ ì „ë¬¸ê°€ë¡œì„œ:**
- ê´€ë ¨ëœ ê³„ì‚°ì´ë‚˜ ìˆ˜ì¹˜ì  ì ‘ê·¼ë²•ì„ ì œì‹œí•˜ì„¸ìš”
- ê°€ì •ì‚¬í•­ì„ ëª…í™•ížˆ í•˜ê³  ê³„ì‚° ê³¼ì •ì„ ì„¤ëª…í•˜ì„¸ìš”
- í†µê³„ì  ê´€ì ì—ì„œì˜ í•´ì„ì„ í¬í•¨í•˜ì„¸ìš”
- ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘ ì‹œ í•„ìš”í•œ ì§€í‘œë¥¼ ì œì•ˆí•˜ì„¸ìš”
- ê²°ê³¼ ê²€ì¦ ë°©ë²•ì„ ì œì‹œí•˜ì„¸ìš”
- ìˆ˜í•™ ê³µì‹ì€ LaTeXë¡œ í‘œí˜„í•˜ì„¸ìš” (ì˜ˆ: $\\mu = \\frac{\\sum x_i}{n}$, $$\\sigma = \\sqrt{\\frac{\\sum (x_i - \\mu)^2}{n}}$$)

ë‹µë³€ì— í¬í•¨í•  ìš”ì†Œ:
- ê¸°ë³¸ ê°€ì •ì‚¬í•­
- ê³„ì‚° ê³µì‹ ë° ë°©ë²• (LaTeX ìˆ˜ì‹ í¬í•¨)
- ì˜ˆì‹œ ê³„ì‚°
- ê²°ê³¼ í•´ì„
- í™œìš© ë°©ì•ˆ"""

        else:
            return base_prompt + """

**ì¢…í•© ì»¨ì„¤í„´íŠ¸ë¡œì„œ:**
- ì§ˆë¬¸ì˜ ë§¥ë½ì„ íŒŒì•…í•˜ê³  í¬ê´„ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”
- ì´ë¡ ì  ì„¤ëª…ê³¼ ì‹¤ë¬´ì  ì ìš© ë°©ë²•ì„ ëª¨ë‘ í¬í•¨í•˜ì„¸ìš”
- ê´€ë ¨ëœ ì‚¬ë¡€ë‚˜ ì˜ˆì‹œë¥¼ ë“¤ì–´ ì„¤ëª…í•˜ì„¸ìš”
- ì‹¤ì œ ë°ì´í„° ë¶„ì„ ì‹œ ë„ì›€ì´ ë  ê´€ì ì„ ì œì‹œí•˜ì„¸ìš”
- í›„ì† ì§ˆë¬¸ì´ë‚˜ ì‹¬í™” ë¶„ì„ ë°©í–¥ì„ ì œì•ˆí•˜ì„¸ìš”
- ìˆ˜í•™ì  ì„¤ëª…ì´ í•„ìš”í•œ ê²½ìš° LaTeX ìˆ˜ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš” (ì˜ˆ: $R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}$)

ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ìž‘ì„±í•˜ë©°, êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì •ë³´ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”."""
    
    async def generate_chat_title(self, first_message: str) -> str:
        """ì‚¬ìš©ìžì˜ ì²« ë©”ì‹œì§€ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì ì ˆí•œ ì±„íŒ… ì œëª©ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            # Check if API key is properly set
            if not settings.OPENAI_API_KEY or settings.OPENAI_API_KEY == "your_openai_api_key_here":
                return "ìƒˆ ì±„íŒ…"
            
            response = await self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system", 
                        "content": """ë‹¹ì‹ ì€ ì±„íŒ… ì œëª©ì„ ìƒì„±í•˜ëŠ” ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. 
ì‚¬ìš©ìžì˜ ì²« ë©”ì‹œì§€ë¥¼ ë¶„ì„í•˜ì—¬ ê°„ê²°í•˜ê³  ì˜ë¯¸ìžˆëŠ” ì±„íŒ… ì œëª©ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

ê·œì¹™:
1. ì œëª©ì€ 3-6ë‹¨ì–´ë¡œ ê°„ê²°í•˜ê²Œ ìž‘ì„±
2. í•µì‹¬ ë‚´ìš©ì„ ëª…í™•ížˆ í‘œí˜„
3. í•œêµ­ì–´ë¡œ ìž‘ì„±
4. íŠ¹ìˆ˜ë¬¸ìžë‚˜ ë”°ì˜´í‘œ ì—†ì´ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œë§Œ ìž‘ì„±
5. ë°ì´í„° ë¶„ì„ ê´€ë ¨ ì§ˆë¬¸ì´ë©´ ë¶„ì„ ì£¼ì œë¥¼ í¬í•¨

ì˜ˆì‹œ:
- ìž…ë ¥: "ë§¤ì¶œ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”" â†’ ì¶œë ¥: "ë§¤ì¶œ ë°ì´í„° ë¶„ì„"
- ìž…ë ¥: "ê³ ê° ë§Œì¡±ë„ëŠ” ì–´ë–»ê²Œ ì¸¡ì •í•˜ë‚˜ìš”?" â†’ ì¶œë ¥: "ê³ ê° ë§Œì¡±ë„ ì¸¡ì • ë°©ë²•"
- ìž…ë ¥: "Pythonìœ¼ë¡œ ì°¨íŠ¸ë¥¼ ê·¸ë¦¬ëŠ” ë°©ë²•" â†’ ì¶œë ¥: "Python ì°¨íŠ¸ ê·¸ë¦¬ê¸°"
"""
                    },
                    {"role": "user", "content": f"ë‹¤ìŒ ë©”ì‹œì§€ì˜ ì œëª©ì„ ìƒì„±í•´ì£¼ì„¸ìš”: {first_message}"}
                ],
                temperature=0.3,
                max_tokens=50
            )
            
            title = response.choices[0].message.content.strip()
            
            # ì œëª©ì´ ë„ˆë¬´ ê¸¸ë©´ ìž˜ë¼ë‚´ê¸° (ìµœëŒ€ 30ìž)
            if len(title) > 30:
                title = title[:30] + "..."
            
            # ë¹ˆ ì œëª©ì´ê±°ë‚˜ ë„ˆë¬´ ì§§ìœ¼ë©´ ê¸°ë³¸ ì œëª© ë°˜í™˜
            if not title or len(title) < 3:
                return "ìƒˆ ì±„íŒ…"
                
            return title
            
        except Exception as e:
            print(f"Chat title generation AI error: {str(e)}")
            return "ìƒˆ ì±„íŒ…"

    async def analyze_with_code_execution(self, df: pd.DataFrame, question: str) -> Dict[str, Any]:
        """ì½”ë“œ ì‹¤í–‰ì„ í†µí•œ ì •í™•í•œ ë°ì´í„° ë¶„ì„"""
        try:
            # ì§ˆë¬¸ì—ì„œ ê³„ì‚°ì´ í•„ìš”í•œì§€ í™•ì¸
            calculation_keywords = ['í‰ê· ', 'average', 'ì´í•©', 'sum', 'ê°œìˆ˜', 'count', 'ìµœëŒ€', 'max', 'ìµœì†Œ', 'min', 'í‘œì¤€íŽ¸ì°¨', 'std']
            needs_calculation = any(keyword in question.lower() for keyword in calculation_keywords)

            if not needs_calculation:
                # ì¼ë°˜ ë¶„ì„ìœ¼ë¡œ ì²˜ë¦¬
                return await self.analyze_data(df, question)

            # ì½”ë“œ ìƒì„±ì„ ìœ„í•œ AI í”„ë¡¬í”„íŠ¸
            code_generation_prompt = f"""
ë°ì´í„° ë¶„ì„ ì§ˆë¬¸: "{question}"

ë‹¤ìŒ ë°ì´í„°ì…‹ì— ëŒ€í•´ ì •í™•í•œ ê³„ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” Python ì½”ë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

ë°ì´í„° ì •ë³´:
- í–‰ ìˆ˜: {len(df):,}
- ì—´ ìˆ˜: {len(df.columns)}
- ì»¬ëŸ¼: {df.columns.tolist()}
- ìƒ˜í”Œ ë°ì´í„°: {df.head(3).to_dict()}

ìš”êµ¬ì‚¬í•­:
1. pandasì™€ numpyë¥¼ ì‚¬ìš©í•œ ì •í™•í•œ ê³„ì‚°
2. ê²°ê³¼ë¥¼ ëª…í™•ížˆ ì¶œë ¥í•˜ëŠ” print ë¬¸ í¬í•¨
3. LaTeX ìˆ˜ì‹ìœ¼ë¡œ ê³µì‹ ì„¤ëª… í¬í•¨
4. ë‹¨ê³„ë³„ë¡œ ë‚˜ëˆ„ì–´ì§„ ì‹¤í–‰ ê°€ëŠ¥í•œ ì½”ë“œ

ì½”ë“œë§Œ ë°˜í™˜í•˜ê³  ì„¤ëª…ì€ ìµœì†Œí™”í•˜ì„¸ìš”.
"""

            # AIì—ê²Œ ì½”ë“œ ìƒì„± ìš”ì²­
            response = await self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "Python ë°ì´í„° ë¶„ì„ ì½”ë“œë¥¼ ìƒì„±í•˜ëŠ” ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. ì •í™•í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì½”ë“œë§Œ ë°˜í™˜í•©ë‹ˆë‹¤."},
                    {"role": "user", "content": code_generation_prompt}
                ],
                temperature=0.1,
                max_tokens=1000
            )

            generated_code = response.choices[0].message.content.strip()

            # ìƒì„±ëœ ì½”ë“œë¥¼ ì²­í¬ë¡œ ë¶„í• 
            code_chunks = self._split_code_into_chunks(generated_code)

            # ì½”ë“œ ì‹¤í–‰ì„ í†µí•œ ì‹¤ì œ ê³„ì‚°
            execution_result = await self._execute_analysis_code(generated_code, df)

            # ê²°ê³¼ í¬ë§·íŒ…
            analysis_result = {
                'answer': execution_result.get('output', ''),
                'code_execution': {
                    'codeChunks': code_chunks,
                    'isExecuting': False,
                    'result': execution_result.get('result', '')
                },
                'insights': [
                    f"âœ… ì½”ë“œ ì‹¤í–‰ì„ í†µí•œ ì •í™•í•œ ê³„ì‚° ê²°ê³¼ìž…ë‹ˆë‹¤.",
                    f"âš¡ ì‹¤í–‰ ì‹œê°„: {execution_result.get('execution_time', 0):.2f}ì´ˆ",
                    "ðŸ“Š ëª¨ë“  ìˆ˜ì¹˜ëŠ” ì‹¤ì œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³„ì‚°ë˜ì—ˆìŠµë‹ˆë‹¤."
                ]
            }

            if execution_result.get('error'):
                analysis_result['insights'].append(f"âš ï¸ ì‹¤í–‰ ì¤‘ ê²½ê³ : {execution_result['error']}")

            return analysis_result

        except Exception as e:
            print(f"Code execution analysis error: {str(e)}")
            # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ë¶„ì„ìœ¼ë¡œ í´ë°±
            return await self.analyze_data(df, question)

    def _split_code_into_chunks(self, code: str) -> List[str]:
        """ì½”ë“œë¥¼ ì‹¤í–‰ ë‹¨ìœ„ë³„ë¡œ ë¶„í• """
        # ë°±í‹± ì œê±°
        code = code.strip()
        if code.startswith('```python'):
            code = code[9:]
        if code.startswith('```'):
            code = code[3:]
        if code.endswith('```'):
            code = code[:-3]

        lines = code.split('\n')
        chunks = []
        current_chunk = []

        for i, line in enumerate(lines):
            current_chunk.append(line)

            # ì˜ë¯¸ìžˆëŠ” ë‹¨ìœ„ë¡œ ì²­í¬ ë¶„í• 
            is_break_point = (
                line.strip().startswith('import ') or  # import ë¬¸
                line.strip().startswith('from ') or   # from import ë¬¸
                line.strip().startswith('#') or       # ì£¼ì„
                (line.strip().startswith('print(') and line.strip().endswith(')')) or  # ì™„ì „í•œ print ë¬¸
                (line.strip() == '' and i > 0 and lines[i-1].strip() != '') or  # ë¹ˆ ì¤„ (ì—°ì† ë¹ˆ ì¤„ ì œì™¸)
                (i == len(lines) - 1)  # ë§ˆì§€ë§‰ ì¤„
            )

            if is_break_point and current_chunk:
                chunk_text = '\n'.join(current_chunk).strip()
                if chunk_text:
                    chunks.append(chunk_text)
                current_chunk = []

        # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
        if current_chunk:
            chunk_text = '\n'.join(current_chunk).strip()
            if chunk_text:
                chunks.append(chunk_text)

        # ë¹ˆ ì²­í¬ ì œê±° ë° ìµœì†Œ 1ê°œ ì²­í¬ ë³´ìž¥
        chunks = [chunk for chunk in chunks if chunk.strip()]
        if not chunks and code.strip():
            chunks = [code.strip()]

        return chunks

    async def _execute_analysis_code(self, code: str, df: pd.DataFrame) -> Dict[str, Any]:
        """ë¶„ì„ ì½”ë“œë¥¼ ì•ˆì „í•˜ê²Œ ì‹¤í–‰"""
        try:
            # ì½”ë“œ ì‹¤í–‰ API í˜¸ì¶œ
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    "http://localhost:8000/api/code/execute",
                    json={
                        "code": code,
                        "context": {
                            "data": df.to_dict(),  # ë°ì´í„°í”„ë ˆìž„ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ì „ë‹¬
                            "df": df.to_dict()  # í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
                        }
                    },
                    timeout=30.0
                )

                if response.status_code == 200:
                    return response.json()
                else:
                    return {
                        "success": False,
                        "output": "",
                        "error": f"ì‹¤í–‰ ì‹¤íŒ¨: {response.status_code}",
                        "execution_time": 0
                    }

        except Exception as e:
            return {
                "success": False,
                "output": "",
                "error": f"ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}",
                "execution_time": 0
            }

    async def unified_analysis(self, question: str, df: pd.DataFrame = None, file_info: dict = None, conversation_history: list = None) -> Dict[str, Any]:
        """
        ChatGPT ìŠ¤íƒ€ì¼ ì™„ì „í•œ ë¶„ì„ ì‹œìŠ¤í…œ:
        1. ë¹ ë¥¸ Python ì½”ë“œ ìƒì„± â†’ 2. ì½”ë“œ ì‹¤í–‰ â†’ 3. Plotly ì°¨íŠ¸ ìƒì„± â†’ 4. AI ì¸ì‚¬ì´íŠ¸ ë¶„ì„
        """
        try:
            print(f"ðŸš€ ChatGPT ìŠ¤íƒ€ì¼ ë¶„ì„ ì‹œìž‘: {question}")

            # 1ë‹¨ê³„: ëŒ€í™” ížˆìŠ¤í† ë¦¬ ê°•í™”
            enhanced_question = self._enhance_question_with_context(question, conversation_history)
            print(f"ðŸ”„ Enhanced question: {enhanced_question}")

            # 2ë‹¨ê³„: ë¹ ë¥¸ Python ì½”ë“œ ìƒì„± (ChatGPT ìŠ¤íƒ€ì¼)
            if df is not None and not df.empty:
                print(f"ðŸ“Š ë°ì´í„° ê¸°ë°˜ ë¶„ì„ (í–‰: {len(df)}, ì—´: {len(df.columns)})")
                generated_code = await self._generate_chatgpt_style_code(enhanced_question, df, file_info)
            else:
                print("ðŸ”¢ ì¼ë°˜ ê³„ì‚°/ë¶„ì„ ëª¨ë“œ")
                generated_code = await self._generate_chatgpt_general_code(enhanced_question)

            print(f"âœ… Python ì½”ë“œ ìƒì„± ì™„ë£Œ")

            # 3ë‹¨ê³„: ì½”ë“œ ì‹¤í–‰ ë° ì°¨íŠ¸ ìƒì„±
            code_chunks = [generated_code] if generated_code else []
            execution_result = ""
            chart_data = None

            if generated_code:
                # ðŸš€ API íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•œ ì•ˆì •ì ì¸ ì½”ë“œ ì‹¤í–‰
                print(f"âš¡ ì½”ë“œ ì‹¤í–‰ ì‹œìž‘ (í†µí•© API íŒŒì´í”„ë¼ì¸)...")
                exec_result = await self._execute_code_via_api(generated_code, df)
                execution_result = exec_result.get('output', '')
                chart_data = exec_result.get('chart_data')

                # ë””ë²„ê¹…: ì‹¤í–‰ ê²°ê³¼ ìƒì„¸ ë¡œê·¸
                print(f"ðŸ” ì½”ë“œ ì‹¤í–‰ ì™„ë£Œ (í†µí•© API):")
                print(f"  - ì„±ê³µ: {exec_result.get('success', False)}")
                print(f"  - ì¶œë ¥ ê¸¸ì´: {len(execution_result) if execution_result else 0}")
                print(f"  - ì¶œë ¥ ë‚´ìš©: {execution_result[:200]}..." if execution_result else "  - ì¶œë ¥ ì—†ìŒ")
                print(f"  - ì°¨íŠ¸ ë°ì´í„° ì¡´ìž¬: {bool(chart_data)}")
                if exec_result.get('error'):
                    print(f"  - ì˜¤ë¥˜: {exec_result.get('error')}")

                if chart_data:
                    print(f"ðŸ“ˆ ì°¨íŠ¸ ë°ì´í„° ìˆ˜ì‹  ì„±ê³µ! í¬ê¸°: {len(str(chart_data))}")
                else:
                    print(f"ðŸ“Š ì°¨íŠ¸ ë°ì´í„° ì—†ìŒ - í…ìŠ¤íŠ¸ ë¶„ì„ìœ¼ë¡œ ì§„í–‰")

            # 4ë‹¨ê³„: AI ì¸ì‚¬ì´íŠ¸ ìƒì„± (ChatGPT ìŠ¤íƒ€ì¼)
            print(f"ðŸ§  AI ì¸ì‚¬ì´íŠ¸ ë¶„ì„ ì‹œìž‘...")
            insights = await self._generate_chatgpt_insights(
                question, execution_result, chart_data, df
            )

            # 5ë‹¨ê³„: í›„ì† ì§ˆë¬¸ ìƒì„±
            follow_up_questions = await self._generate_follow_up_questions(question, {
                'output': execution_result,
                'chart_data': chart_data
            })

            # 6ë‹¨ê³„: ChatGPT ìŠ¤íƒ€ì¼ ì™„ì „í•œ ë‹µë³€ ìƒì„±
            comprehensive_answer = await self._generate_comprehensive_answer(
                question, execution_result, insights, chart_data
            )

            result = {
                'answer': comprehensive_answer,
                'code_execution': {
                    'codeChunks': code_chunks,
                    'isExecuting': False,  # ì‹¤í–‰ ì™„ë£Œ
                    'result': execution_result,
                    'output': execution_result
                },
                'insights': insights,
                'followUpQuestions': follow_up_questions,
                'chartData': chart_data
            }

            print(f"ðŸŽ‰ ChatGPT ìŠ¤íƒ€ì¼ ì™„ì „í•œ ë¶„ì„ ì™„ë£Œ!")
            return convert_numpy_types(result)

        except Exception as e:
            import traceback
            print(f"âŒ Unified analysis error: {str(e)}")
            print(f"Full traceback: {traceback.format_exc()}")
            return {
                'answer': f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                'code_execution': {
                    'codeChunks': [],
                    'isExecuting': False,
                    'result': f"ì˜¤ë¥˜: {str(e)}"
                },
                'insights': [],
                'followUpQuestions': []
            }

    async def _generate_data_analysis_code(self, question: str, df: pd.DataFrame, file_info: dict) -> Dict[str, Any]:
        """íŒŒì¼ì´ ìžˆëŠ” ê²½ìš° ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ë¶„ì„ ì½”ë“œ ìƒì„± - ìŠ¤ë§ˆíŠ¸ ì»¬ëŸ¼ ë§¤í•‘"""

        # AI ê¸°ë°˜ ìœ ì—°í•œ ì½”ë“œ ìƒì„±
        columns = df.columns.tolist()

        # ì»¬ëŸ¼ ì •ë³´ ìˆ˜ì§‘
        column_info = {}
        for col in columns:
            try:
                unique_values = df[col].dropna().unique()
                sample_values = [convert_numpy_types(val) for val in unique_values[:5]] if len(unique_values) > 0 else []
                column_info[col] = {
                    'type': str(df[col].dtype),
                    'sample_values': sample_values,
                    'unique_count': convert_numpy_types(len(unique_values)),
                    'null_count': convert_numpy_types(df[col].isnull().sum())
                }
            except:
                column_info[col] = {'type': 'unknown', 'sample_values': [], 'unique_count': 0, 'null_count': 0}

        # AIì—ê²Œ ì§ì ‘ ì½”ë“œ ìƒì„± ìš”ì²­
        generated_code = await self._generate_flexible_analysis_code(question, df, column_info)
        analysis_type = "ai_generated"

        # ê¸°ì¡´ í•˜ë“œì½”ë”©ëœ ë¶„ì„ íƒ€ìž…ë“¤ì„ ì£¼ì„ ì²˜ë¦¬í•˜ê³  AI ìƒì„±ìœ¼ë¡œ ëŒ€ì²´
        # if column_mapping.get("analysis_type") == "gender_analysis" and column_mapping.get("target_column"):
        #     sex_column = column_mapping["target_column"]
        # analysis_type = "gender_analysis"
        # generated_code = f"""import pandas as pd
        # import plotly.express as px
        # import plotly.graph_objects as go
        # import json

# ì„±ë³„ ë¶„í¬ ë¶„ì„
        # print("=== ì„±ë³„ ë¶„í¬ ë¶„ì„ ===")
        # sex_counts = df['{sex_column}'].value_counts()
        # print("ì„±ë³„ë³„ ê°œì²´ ìˆ˜:")
        # for gender, cnt in sex_counts.items():
        # percentage = (cnt / len(df)) * 100
        # print(f"  {gender}: {cnt:,}ë§ˆë¦¬ ({percentage:.1f}%)")

        # print(f"ì´ ë¶„ì„ ê°œì²´ ìˆ˜: {len(df):,}ë§ˆë¦¬")

# Plotly ë°” ì°¨íŠ¸ ìƒì„±
        # fig = px.bar(
        # x=sex_counts.index,
        # y=sex_counts.values,
        # labels={{'x': 'ì„±ë³„', 'y': 'ê°œì²´ ìˆ˜'}},
        # title='{sex_column} ê¸°ì¤€ ê°œì²´ ìˆ˜ ë¶„í¬',
        # color=sex_counts.values,
        # color_continuous_scale='viridis'
        # )

        # fig.update_layout(
        # xaxis_title='ì„±ë³„',
        # yaxis_title='ê°œì²´ ìˆ˜',
        # showlegend=False,
        # height=500
        # )

# ì°¨íŠ¸ ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ë³€í™˜
        # chart_json = fig.to_json()
        # print("ðŸ“Š ì°¨íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")"""

        # ë‚˜ì´/ì—°ë ¹ ë¶„ì„
        # elif column_mapping.get("analysis_type") == "age_analysis" and column_mapping.get("target_column"):
        # age_column = column_mapping["target_column"]
        # analysis_type = "age_analysis"
        # generated_code = f"""import pandas as pd
        # import plotly.express as px
        # import numpy as np

# ì—°ë ¹ ë¶„í¬ ë¶„ì„
        # print("=== ì—°ë ¹ ë¶„í¬ ë¶„ì„ ===")
        # ages = df['{age_column}'].dropna()
        # print(f"í‰ê·  ì—°ë ¹: {ages.mean():.1f}ì„¸")
        # print(f"ì—°ë ¹ ë²”ìœ„: {ages.min():.0f}ì„¸ - {ages.max():.0f}ì„¸")
        # print(f"í‘œì¤€íŽ¸ì°¨: {ages.std():.1f}ì„¸")

# ížˆìŠ¤í† ê·¸ëž¨ ìƒì„±
        # fig = px.histogram(
        # df,
        # x='{age_column}',
        # nbins=20,
        # title='ì—°ë ¹ ë¶„í¬',
        # labels={{'{age_column}': 'ì—°ë ¹', 'count': 'ê°œì²´ ìˆ˜'}}
        # )

        # fig.update_layout(
        # xaxis_title='ì—°ë ¹',
        # yaxis_title='ê°œì²´ ìˆ˜',
        # height=500
        # )

        # chart_json = fig.to_json()
        # print("\\nðŸ“Š ì—°ë ¹ ížˆìŠ¤í† ê·¸ëž¨ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")"""

        # ì§€ì—­/ì§€ì—­ë³„ ë¶„ì„
        # elif column_mapping.get("analysis_type") == "location_analysis" and column_mapping.get("target_column"):
        # location_column = column_mapping["target_column"]
        # analysis_type = "location_analysis"
        # generated_code = f"""import pandas as pd
        # import plotly.express as px

# ì§€ì—­ë³„ ë¶„í¬ ë¶„ì„
        # print("=== ì§€ì—­ë³„ ë¶„í¬ ë¶„ì„ ===")
        # location_counts = df['{location_column}'].value_counts()
        # print("ì§€ì—­ë³„ ê°œì²´ ìˆ˜:")
        # for location, count in location_counts.head(10).items():
        # percentage = (count / len(df)) * 100
        # print(f"  {location}: {count:,}ë§ˆë¦¬ ({percentage:.1f}%)")

        # if len(location_counts) > 10:
        # print(f"... ë° {len(location_counts) - 10}ê°œ ì§€ì—­ ë”")

# ë°” ì°¨íŠ¸ ìƒì„± (ìƒìœ„ 15ê°œ ì§€ì—­)
        # top_locations = location_counts.head(15)
        # fig = px.bar(
        # x=top_locations.values,
        # y=top_locations.index,
        # orientation='h',
        # title='ì§€ì—­ë³„ ê°œì²´ ìˆ˜ ë¶„í¬ (ìƒìœ„ 15ê°œ)',
        # labels={{'x': 'ê°œì²´ ìˆ˜', 'y': 'ì§€ì—­'}}
        # )

        # fig.update_layout(height=600, yaxis_title='ì§€ì—­', xaxis_title='ê°œì²´ ìˆ˜')
        # chart_json = fig.to_json()
        # print("\\nðŸ“Š ì§€ì—­ë³„ ë¶„í¬ ì°¨íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")"""

        # í¬ê¸°/ê¸¸ì´/ë¬´ê²Œ ë¶„ì„
        # elif column_mapping.get("analysis_type") == "size_analysis" and column_mapping.get("target_column"):
        # size_column = column_mapping["target_column"]
        # analysis_type = "size_analysis"
        # generated_code = f"""import pandas as pd
        # import plotly.express as px
        # import numpy as np

# í¬ê¸° ë¶„í¬ ë¶„ì„
        # print("=== {size_column} ë¶„í¬ ë¶„ì„ ===")
        # sizes = df['{size_column}'].dropna()
        # print(f"í‰ê· : {sizes.mean():.2f}")
        # print(f"ì¤‘ì•™ê°’: {sizes.median():.2f}")
        # print(f"ë²”ìœ„: {sizes.min():.2f} - {sizes.max():.2f}")
        # print(f"í‘œì¤€íŽ¸ì°¨: {sizes.std():.2f}")

# ížˆìŠ¤í† ê·¸ëž¨ ìƒì„±
        # fig = px.histogram(
        # df,
        # x='{size_column}',
        # nbins=30,
        # title=f'{size_column} ë¶„í¬',
        # labels={{'{size_column}': '{size_column}', 'count': 'ê°œì²´ ìˆ˜'}}
        # )

        # fig.update_layout(height=500)
        # chart_json = fig.to_json()
        # print("\\nðŸ“Š í¬ê¸° ë¶„í¬ ížˆìŠ¤í† ê·¸ëž¨ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")"""

        # ì¼ë°˜ì ì¸ ë°ì´í„° ìš”ì•½ (ê¸°ë³¸ê°’)
        # else:
        # analysis_type = "general_summary"
        # generated_code = f"""import pandas as pd
        # import plotly.express as px

# ë°ì´í„° ì „ì²´ ìš”ì•½
        # print("=== ë°ì´í„° ìš”ì•½ ===")
        # print(f"ì´ í–‰ ìˆ˜: {{len(df):,}}")
        # print(f"ì´ ì»¬ëŸ¼ ìˆ˜: {{len(df.columns)}}")

        # print("\\nì»¬ëŸ¼ ëª©ë¡:")
        # for i, col in enumerate(df.columns, 1):
        # print(f"  {{i}}. {{col}}")

# ìˆ«ìží˜• ì»¬ëŸ¼ í†µê³„
        # numeric_cols = df.select_dtypes(include=['number']).columns
        # if len(numeric_cols) > 0:
        # print("\\nìˆ«ìží˜• ì»¬ëŸ¼ í†µê³„:")
        # for col in numeric_cols[:5]:  # ìµœëŒ€ 5ê°œ
        # values = df[col].dropna()
        # if len(values) > 0:
        # print(f"  {{col}}: í‰ê·  {{values.mean():.2f}}, ë²”ìœ„ {{values.min():.2f}}-{{values.max():.2f}}")

# ì²« ë²ˆì§¸ ë²”ì£¼í˜• ì»¬ëŸ¼ìœ¼ë¡œ ê°„ë‹¨í•œ ì°¨íŠ¸ ìƒì„±
        # categorical_cols = df.select_dtypes(include=['object']).columns
        # if len(categorical_cols) > 0:
        # chart_col = categorical_cols[0]
        # value_counts = df[chart_col].value_counts().head(10)

        # fig = px.bar(
        # x=value_counts.index,
        # y=value_counts.values,
        # title=f'{{chart_col}} ë¶„í¬ (ìƒìœ„ 10ê°œ)',
        # labels={{'x': chart_col, 'y': 'ê°œìˆ˜'}}
        # )

        # fig.update_layout(height=500)
        # chart_json = fig.to_json()
        # print(f"\\nðŸ“Š {{chart_col}} ë¶„í¬ ì°¨íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
        # else:
        # print("\\nì°¨íŠ¸ ìƒì„±ì„ ìœ„í•œ ì ì ˆí•œ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")"""

        # ì½”ë“œê°€ ìƒì„±ë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ ì½”ë“œ
        # if not generated_code:
        # generated_code = """print("ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ë¶„ì„ ì½”ë“œë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        # print("ë°ì´í„°ì˜ ì»¬ëŸ¼ ëª©ë¡ì„ í™•ì¸í•´ì£¼ì„¸ìš”:")
        # for i, col in enumerate(df.columns, 1):
        # print(f"  {i}. {col}")"""

        # ìƒì„±ëœ ì½”ë“œë¥¼ ì²­í¬ë¡œ ë¶„í• 
        code_chunks = [generated_code] if generated_code else []

        # ì‹¤ì œ ë°ì´í„°ë¡œ ì½”ë“œ ì‹¤í–‰
        execution_result = await self._execute_analysis_code_with_data(generated_code, df)

        # ì‹¤í–‰ ê²°ê³¼ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ìƒì„±
        insights = self._generate_insights_from_results(execution_result, analysis_type, df, question)

        return {
            'output': execution_result.get('output', ''),
            'code_chunks': code_chunks,
            'result': execution_result.get('result', ''),
            'insights': insights,
            'chart_data': execution_result.get('chart_data')
        }

    def _generate_insights_from_results(self, execution_result: Dict[str, Any], analysis_type: str, df: pd.DataFrame, question: str) -> List[str]:
        """ì‹¤í–‰ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì˜ë¯¸ìžˆëŠ” ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = []

        # ê¸°ë³¸ ì‹¤í–‰ ì •ë³´
        if execution_result.get('success', True):
            insights.append(f"âœ… ì´ {len(df):,}í–‰ì˜ ì‹¤ì œ ë°ì´í„°ë¥¼ ë¶„ì„í–ˆìŠµë‹ˆë‹¤.")

            execution_time = execution_result.get('execution_time', 0)
            if execution_time > 0:
                insights.append(f"âš¡ ë¶„ì„ ì‹¤í–‰ ì‹œê°„: {execution_time:.3f}ì´ˆ")
        else:
            insights.append("âŒ ì½”ë“œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            return insights

        # ë¶„ì„ ê²°ê³¼ì—ì„œ ìˆ«ìž ì¶”ì¶œ ì‹œë„
        output = execution_result.get('output', '')

        if analysis_type == "gender_analysis":
            insights.append("ðŸ” ì„±ë³„ ë¶„í¬ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

            # ì¶œë ¥ì—ì„œ ë¹„ìœ¨ ì •ë³´ ì¶”ì¶œ ì‹œë„
            if '%' in output:
                insights.append("ðŸ“Š ê° ì„±ë³„ì˜ ë¹„ìœ¨ê³¼ ê°œì²´ ìˆ˜ê°€ ê³„ì‚°ë˜ì—ˆìŠµë‹ˆë‹¤.")

            if 'Female' in output or 'Male' in output or 'ì•”ì»·' in output or 'ìˆ˜ì»·' in output:
                insights.append("â™‚â™€ ìˆ˜ì»·ê³¼ ì•”ì»·ì˜ ë¶„í¬ë¥¼ ì‹œê°ì ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.")

        elif analysis_type == "age_analysis":
            insights.append("ðŸ“ˆ ì—°ë ¹ ë¶„í¬ í†µê³„ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
            if 'í‰ê· ' in output:
                insights.append("ðŸ“Š í‰ê·  ì—°ë ¹, ë²”ìœ„, í‘œì¤€íŽ¸ì°¨ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.")

        elif analysis_type == "location_analysis":
            insights.append("ðŸŒ ì§€ì—­ë³„ ë¶„í¬ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            if 'ì§€ì—­' in output:
                insights.append("ðŸ“ ìƒìœ„ ì§€ì—­ë“¤ì˜ ê°œì²´ ìˆ˜ ë¶„í¬ë¥¼ í™•ì¸í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.")

        elif analysis_type == "size_analysis":
            insights.append("ðŸ“ í¬ê¸°/ê¸¸ì´ ë¶„í¬ í†µê³„ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
            if 'í‰ê· ' in output:
                insights.append("ðŸ“Š í‰ê· ê°’, ì¤‘ì•™ê°’, ë²”ìœ„ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.")

        else:
            insights.append("ðŸ“‹ ë°ì´í„° ì „ë°˜ì ì¸ ìš”ì•½ ì •ë³´ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

        # ì°¨íŠ¸ ìƒì„± ì—¬ë¶€ í™•ì¸
        if execution_result.get('chart_data') or 'ì°¨íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤' in output:
            insights.append("ðŸ“Š ëŒ€í™”í˜• ê·¸ëž˜í”„ê°€ ìƒì„±ë˜ì–´ ì‹œê°ì  ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")

        # ë°ì´í„° í’ˆì§ˆ ê´€ë ¨ ì¸ì‚¬ì´íŠ¸
        if len(df) >= 1000:
            insights.append("ðŸ’ª ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ìœ¼ë¡œ í†µê³„ì ìœ¼ë¡œ ì‹ ë¢°ì„± ìžˆëŠ” ë¶„ì„ìž…ë‹ˆë‹¤.")
        elif len(df) >= 100:
            insights.append("ðŸ“ˆ ì¶©ë¶„í•œ ìƒ˜í”Œ í¬ê¸°ë¡œ ì˜ë¯¸ìžˆëŠ” ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")

        return insights

    async def _smart_column_mapping(self, question: str, columns: List[str], df: pd.DataFrame) -> Dict[str, str]:
        """AIë¥¼ í™œìš©í•œ ìŠ¤ë§ˆíŠ¸ ì»¬ëŸ¼ ë§¤í•‘ - ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ íŒŒì•…í•˜ì—¬ ì ì ˆí•œ ì»¬ëŸ¼ì„ ì°¾ìŒ"""

        # ìƒ˜í”Œ ë°ì´í„° ì¤€ë¹„ (ê° ì»¬ëŸ¼ì˜ ê³ ìœ ê°’ ëª‡ ê°œì”©)
        column_info = {}
        for col in columns:
            try:
                unique_values = df[col].dropna().unique()
                if len(unique_values) > 0:
                    # ë„ˆë¬´ ë§Žìœ¼ë©´ ì²˜ìŒ 5ê°œë§Œ
                    sample_values = [convert_numpy_types(val) for val in unique_values[:5]]
                    column_info[col] = {
                        'type': str(df[col].dtype),
                        'sample_values': sample_values,
                        'unique_count': convert_numpy_types(len(unique_values))
                    }
            except:
                column_info[col] = {'type': 'unknown', 'sample_values': [], 'unique_count': 0}

        # AIì—ê²Œ ì»¬ëŸ¼ ë§¤í•‘ ìš”ì²­
        mapping_prompt = f"""
ì‚¬ìš©ìžê°€ ë°ì´í„°ì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì´ ì§ˆë¬¸í–ˆìŠµë‹ˆë‹¤: "{question}"

ë°ì´í„°ì˜ ì»¬ëŸ¼ ì •ë³´:
{json.dumps(convert_numpy_types(column_info), ensure_ascii=False, indent=2)}

ì‚¬ìš©ìžì˜ ì§ˆë¬¸ ì˜ë„ë¥¼ íŒŒì•…í•˜ì—¬ ë‹¤ìŒ ë¶„ì„ ìœ í˜• ì¤‘ í•˜ë‚˜ì™€ í•´ë‹¹í•˜ëŠ” ì»¬ëŸ¼ì„ ë§¤í•‘í•´ì£¼ì„¸ìš”:

ë¶„ì„ ìœ í˜•:
1. gender_analysis: ì„±ë³„, ìˆ˜ì»·/ì•”ì»·, ë‚¨ë…€ ê´€ë ¨ ë¶„ì„
2. age_analysis: ë‚˜ì´, ì—°ë ¹, ì‚´ ê´€ë ¨ ë¶„ì„
3. location_analysis: ì§€ì—­, ë‚˜ë¼, êµ­ê°€, ìœ„ì¹˜ ê´€ë ¨ ë¶„ì„
4. size_analysis: í¬ê¸°, ê¸¸ì´, ë¬´ê²Œ, í‚¤, ëª¸ë¬´ê²Œ, ì²´ì¤‘, ì¤‘ëŸ‰, í‘œì¤€íŽ¸ì°¨, í‰ê· , ë¶„ì‚° ê´€ë ¨ ë¶„ì„
5. general_summary: ìœ„ì˜ ì¹´í…Œê³ ë¦¬ì— í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ì¼ë°˜ì ì¸ ë¶„ì„

ì‘ë‹µ í˜•ì‹ (JSON):
{{
  "analysis_type": "ë¶„ì„_ìœ í˜•",
  "target_column": "í•´ë‹¹_ì»¬ëŸ¼ëª…_ë˜ëŠ”_null",
  "confidence": 0.8,
  "reasoning": "ì„ íƒí•œ ì´ìœ "
}}

ì£¼ì˜ì‚¬í•­:
- í•œêµ­ì–´ ì§ˆë¬¸ì´ì–´ë„ ì˜ì–´ ì»¬ëŸ¼ëª…ê³¼ ì˜ë¯¸ì ìœ¼ë¡œ ë§¤ì¹­í•´ì£¼ì„¸ìš”
- ìƒ˜í”Œ ê°’ì„ ë³´ê³  ì»¬ëŸ¼ì˜ ì‹¤ì œ ë‚´ìš©ì„ íŒë‹¨í•˜ì„¸ìš”
- í™•ì‹ ì´ ì—†ìœ¼ë©´ confidenceë¥¼ ë‚®ê²Œ ì„¤ì •í•˜ì„¸ìš”
"""

        try:
            response = await self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ë¡œì„œ ì‚¬ìš©ìžì˜ ì§ˆë¬¸ ì˜ë„ë¥¼ ì •í™•ížˆ íŒŒì•…í•˜ì—¬ ì ì ˆí•œ ì»¬ëŸ¼ì„ ë§¤í•‘í•©ë‹ˆë‹¤."},
                    {"role": "user", "content": mapping_prompt}
                ],
                temperature=0.1,
                max_tokens=500
            )

            mapping_result = response.choices[0].message.content.strip()

            # JSON íŒŒì‹± ì‹œë„

            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
            json_match = re.search(r'\{.*\}', mapping_result, re.DOTALL)
            if json_match:
                mapping_data = json.loads(json_match.group())
                return mapping_data
            else:
                return {"analysis_type": "general_summary", "target_column": None, "confidence": 0.0, "reasoning": "JSON íŒŒì‹± ì‹¤íŒ¨"}

        except Exception as e:
            print(f"Smart column mapping error: {e}")
            return {"analysis_type": "general_summary", "target_column": None, "confidence": 0.0, "reasoning": f"ì˜¤ë¥˜: {str(e)}"}

    def _enhance_question_with_context(self, question: str, conversation_history: list = None) -> str:
        """ëŒ€í™” ížˆìŠ¤í† ë¦¬ë¥¼ í™œìš©í•˜ì—¬ ì§ˆë¬¸ì„ ë§¥ë½ì ìœ¼ë¡œ ê°•í™”"""
        if not conversation_history or len(conversation_history) == 0:
            return question

        # ìµœê·¼ 3ê°œ ëŒ€í™”ë§Œ ì‚¬ìš© (í† í° í•œë„ ê³ ë ¤)
        recent_history = conversation_history[-6:] if len(conversation_history) > 6 else conversation_history

        # ì°¸ì¡° ë‹¨ì–´ë“¤ ì²´í¬ ("ì´ê²ƒ", "ê·¸ê²ƒ", "ì´ ë°ì´í„°", "ìœ„ì˜ ê²°ê³¼" ë“±)
        reference_words = [
            "ì´ê²ƒ", "ê·¸ê²ƒ", "ì´ê±°", "ê·¸ê±°", "ì´", "ê·¸",
            "ì´ ë°ì´í„°", "ì´ íŒŒì¼", "ì´ ê²°ê³¼", "ìœ„ì˜", "ì•žì„œ",
            "ë°©ê¸ˆ", "ì§ì „", "ì´ì „", "ë‹¤ì‹œ", "ë˜", "ì¶”ê°€ë¡œ",
            "it", "this", "that", "these", "those", "above", "previous"
        ]

        has_reference = any(word in question.lower() for word in reference_words)

        # ëŒ€í™” ë§¥ë½ì„ í¬í•¨í•œ ê°•í™”ëœ ì§ˆë¬¸ ìƒì„± (ì°¸ì¡° ë‹¨ì–´ ìœ ë¬´ì™€ ê´€ê³„ì—†ì´)
        context_summary = ""

        # ëŒ€í™” ížˆìŠ¤í† ë¦¬ì—ì„œ ë§¥ë½ ì •ë³´ ì¶”ì¶œ (ëª¨ë“  ë©”ì‹œì§€ í¬í•¨)
        conversation_context = []
        for msg in recent_history:
            # ConversationMessage ê°ì²´ì¸ ê²½ìš° ì†ì„±ìœ¼ë¡œ ì ‘ê·¼
            if hasattr(msg, 'role'):
                role = getattr(msg, 'role', '')
                content = getattr(msg, 'content', '')
            # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°
            elif isinstance(msg, dict):
                role = msg.get('role', '')
                content = msg.get('content', '')
            else:
                continue

            if role == 'user':
                # ì‚¬ìš©ìž ì§ˆë¬¸ì€ ê°„ëžµí•˜ê²Œ ìš”ì•½ (ì•ˆì „í•œ ë¬¸ìžì—´ ì²˜ë¦¬)
                safe_content = str(content).replace('\n', ' ').replace('\r', ' ')
                summary = safe_content[:150] + "..." if len(safe_content) > 150 else safe_content
                conversation_context.append(f"ì‚¬ìš©ìž: {summary}")
            elif role == 'assistant':
                # AI ë‹µë³€ë„ í¬í•¨í•˜ë˜ ê°„ëžµí•˜ê²Œ (ì•ˆì „í•œ ë¬¸ìžì—´ ì²˜ë¦¬)
                safe_content = str(content).replace('\n', ' ').replace('\r', ' ')
                # íŠ¹ìˆ˜ ë¬¸ìžë‚˜ ê¸´ ìˆ«ìž ë°°ì—´ ì œê±°
                if len(safe_content) > 500:
                    safe_content = safe_content[:500] + "..."
                summary = safe_content[:200] + "..." if len(safe_content) > 200 else safe_content
                conversation_context.append(f"AI: {summary}")

        # ëŒ€í™” ë§¥ë½ì„ í¬í•¨í•œ ì§ˆë¬¸ êµ¬ì„±
        if conversation_context:
            # ìµœê·¼ 4ê°œ ëŒ€í™”ë§Œ ì‚¬ìš© (í† í° í•œë„ ê³ ë ¤)
            recent_context = conversation_context[-4:]
            context_summary = f"ì´ì „ ëŒ€í™” ë§¥ë½:\n{chr(10).join(recent_context)}\n\n"
        else:
            context_summary = ""

        enhanced_question = f"{context_summary}í˜„ìž¬ ì§ˆë¬¸: {question}"

        print(f"ðŸ“ Context enhancement applied - Reference words found: {has_reference}")
        return enhanced_question

    async def _generate_flexible_analysis_code(self, question: str, df: pd.DataFrame, column_info: dict) -> str:
        """AIê°€ ì§ì ‘ Python ë¶„ì„ ì½”ë“œë¥¼ ìƒì„±í•˜ëŠ” ìœ ì—°í•œ ì‹œìŠ¤í…œ"""

        # ë°ì´í„° ìš”ì•½ ì •ë³´ ìƒì„±
        data_summary = f"""
ë°ì´í„°ì…‹ ì •ë³´:
- ì´ í–‰ ìˆ˜: {len(df):,}
- ì´ ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}
- ì»¬ëŸ¼ ì •ë³´: {json.dumps(convert_numpy_types(column_info), ensure_ascii=False, indent=2)}

ìƒ˜í”Œ ë°ì´í„° (ì²˜ìŒ 3í–‰):
{convert_numpy_types(df.head(3).to_dict('records'))}
"""

        analysis_prompt = f"""
ì‚¬ìš©ìž ì§ˆë¬¸: "{question}"

{data_summary}

ìœ„ ë°ì´í„°ì— ëŒ€í•´ ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— ì •í™•ížˆ ë‹µí•˜ëŠ” Python ì½”ë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

í•„ìˆ˜ ìš”êµ¬ì‚¬í•­:
1. import pandas as pd, import plotly.express as px í¬í•¨
2. ë°ì´í„°ëŠ” ì´ë¯¸ 'df' ë³€ìˆ˜ë¡œ ë¡œë“œë˜ì–´ ìžˆìŒ
3. ì§ˆë¬¸ì— ë§žëŠ” ì ì ˆí•œ ì»¬ëŸ¼ì„ ìžë™ìœ¼ë¡œ ì„ íƒ
4. ë¶„ì„ ê²°ê³¼ë¥¼ ëª…í™•í•˜ê²Œ printë¡œ ì¶œë ¥
5. ê°€ëŠ¥í•˜ë©´ ì‹œê°í™”(plotly) í¬í•¨í•˜ê³  ë‹¤ìŒ ì½”ë“œë¡œ ì•ˆì „í•˜ê²Œ ì €ìž¥:
   try:
       chart_json = fig.to_json()
   except Exception as e:
       print(f"Chart JSON generation error: {e}")
       chart_json = None
6. í•œêµ­ì–´ë¡œ ì¶œë ¥ ë©”ì‹œì§€ ìž‘ì„±
7. ì»¬ëŸ¼ëª…ì´ ì˜ì–´ì—¬ë„ ì˜ë¯¸ë¥¼ íŒŒì•…í•´ì„œ ë¶„ì„

ë¶„ì„ íƒ€ìž… ì˜ˆì‹œ:
- ì„±ë³„/gender ë¶„í¬: ë§‰ëŒ€ ì°¨íŠ¸
- ì—°ë ¹/age ë¶„í¬: ížˆìŠ¤í† ê·¸ëž¨
- ì§€ì—­/location ë¶„í¬: ë§‰ëŒ€ ì°¨íŠ¸
- ìˆ˜ì¹˜ í†µê³„: í‰ê· , í‘œì¤€íŽ¸ì°¨, ížˆìŠ¤í† ê·¸ëž¨
- ìƒê´€ê´€ê³„: scatter plot
- ì‹œê³„ì—´: line plot

ì½”ë“œë§Œ ë°˜í™˜í•˜ê³  ì„¤ëª…ì€ ìƒëžµí•˜ì„¸ìš”.
"""

        try:
            response = await self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— ë§žëŠ” ì •í™•í•œ Python ë¶„ì„ ì½”ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."},
                    {"role": "user", "content": analysis_prompt}
                ],
                temperature=0.1,
                max_tokens=2000
            )

            generated_code = response.choices[0].message.content.strip()

            # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
            if '```python' in generated_code:
                generated_code = generated_code.split('```python')[1].split('```')[0]
            elif '```' in generated_code:
                generated_code = generated_code.split('```')[1].split('```')[0]

            generated_code = generated_code.strip()

            print(f"ðŸ¤– AI generated flexible analysis code:")
            print(f"Code preview: {generated_code[:200]}...")

            return generated_code

        except Exception as e:
            print(f"AI code generation error: {e}")
            # í´ë°±: ê¸°ë³¸ ë°ì´í„° ìš”ì•½ ì½”ë“œ
            return f"""import pandas as pd
import plotly.express as px

print("=== ë°ì´í„° ë¶„ì„ ===")
print(f"ì´ {len(df):,}ê°œ í–‰, {len(df.columns)}ê°œ ì»¬ëŸ¼")
print("\\nê¸°ë³¸ í†µê³„:")
print(df.describe())

print("\\nðŸ“Š ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")"""

    async def _generate_general_analysis_code(self, question: str) -> Dict[str, Any]:
        """íŒŒì¼ì´ ì—†ëŠ” ê²½ìš° ì¼ë°˜ì ì¸ ë¶„ì„ ì½”ë“œ ìƒì„±"""

        # ì§ˆë¬¸ì—ì„œ ìˆ«ìžì™€ ì—°ì‚°ìž ì¶”ì¶œ
        import re

        # ìˆ˜ì‹ íŒ¨í„´ ê°ì§€
        math_expression_pattern = r'[\d+\-*\/\(\)\.\s]+'
        has_math_expression = bool(re.search(r'[\d\s]*[\+\-\*\/][\d\s]*', question))

        if has_math_expression:
            # ìˆ˜ì‹ì´ ìžˆëŠ” ê²½ìš°, ì „ì²´ ìˆ˜ì‹ì„ ì¶”ì¶œ
            expression_match = re.search(r'[0-9+\-*\/\(\)\.\s]+', question)
            math_expression = expression_match.group(0).strip() if expression_match else None
        else:
            # ë‹¨ìˆœ ìˆ«ìž ëª©ë¡ì¸ ê²½ìš°
            math_expression = None

        numbers = re.findall(r'\d+(?:\.\d+)?', question)

        if math_expression:
            # ìˆ˜ì‹ì—ì„œ ìž˜ëª»ëœ ì—°ì‚°ìž íŒ¨í„´ ìˆ˜ì •
            clean_expression = math_expression.replace('+*', '*').replace('-*', '*').replace('**', '*')
            # ë” í™•ì‹¤í•œ ì½”ë“œ ìƒì„±ì„ ìœ„í•´ ì§ì ‘ êµ¬ì„±
            direct_code = f'result = eval("{clean_expression}")\nprint(f"ê²°ê³¼: {{result}}")'

            code_prompt = f"""You must return exactly this Python code with NO changes, NO additions, NO markdown:

{direct_code}

Return ONLY the above code. Nothing else."""
        else:
            # ìˆ«ìž ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜
            int_numbers = [int(num) for num in numbers if num.isdigit()]
            direct_code = f'data = {int_numbers}\nresult = sum(data)\nprint(f"ì´í•©: {{result:,}}")'

            code_prompt = f"""You must return exactly this Python code with NO changes, NO additions, NO markdown:

{direct_code}

Return ONLY the above code. Nothing else."""

        # AIì— ì˜ì¡´í•˜ì§€ ì•Šê³  ì§ì ‘ ì½”ë“œ ìƒì„± (ë” ì•ˆì •ì )
        if math_expression:
            generated_code = f'result = eval("{clean_expression}")\nprint(f"ê²°ê³¼: {{result}}")'
        else:
            # ìˆ«ìž ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜
            int_numbers = [int(num) for num in numbers if num.isdigit()]
            generated_code = f'data = {int_numbers}\nresult = sum(data)\nprint(f"ì´í•©: {{result:,}}")'

        print(f"ì§ì ‘ ìƒì„±ëœ ì½”ë“œ:\n{generated_code}")

        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ê³¼ ì„¤ëª… í…ìŠ¤íŠ¸ ì œê±°
        def clean_code(code_text: str) -> str:
            """AIê°€ ìƒì„±í•œ í…ìŠ¤íŠ¸ì—ì„œ ìˆœìˆ˜ Python ì½”ë“œë§Œ ì¶”ì¶œ"""
            # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±° (ë” ê°•ë ¥í•œ ì œê±°)
            if '```python' in code_text:
                code_text = code_text.split('```python')[1].split('```')[0]
            elif '```' in code_text:
                parts = code_text.split('```')
                if len(parts) >= 3:
                    code_text = parts[1]

            # ë¶ˆí•„ìš”í•œ ì„¤ëª… í…ìŠ¤íŠ¸ ì™„ì „ ì œê±°
            if 'CRITICAL:' in code_text:
                parts = code_text.split('CRITICAL:')[0].strip()
                if parts:
                    code_text = parts

            if 'Required format:' in code_text:
                parts = code_text.split('Required format:')[1].strip()
                if parts:
                    code_text = parts

            lines = code_text.split('\n')
            code_lines = []

            # ì¤‘ë³µëœ print ë¬¸ ë°©ì§€ë¥¼ ìœ„í•œ ì„¸íŠ¸
            seen_prints = set()

            for line in lines:
                stripped = line.strip()

                # ë¹ˆ ì¤„ ê±´ë„ˆë›°ê¸°
                if not stripped:
                    continue

                # ì„¤ëª…ì„± í…ìŠ¤íŠ¸ ì™„ì „ ì œê±°
                if (not any(c in stripped for c in ['=', '(', '[']) and
                    any(ord(char) >= 0xAC00 and ord(char) <= 0xD7A3 for char in stripped)):
                    continue

                # Python ì½”ë“œê°€ ì•„ë‹Œ ê²ƒë“¤ ì œê±°
                if (stripped.startswith('Task:') or
                    stripped.startswith('Numbers:') or
                    stripped.startswith('Calculate') or
                    stripped.startswith('CRITICAL:') or
                    stripped.startswith('Required format:')):
                    continue

                # ì¤‘ë³µëœ print ë¬¸ ì œê±°
                if stripped.startswith('print('):
                    if stripped in seen_prints:
                        continue
                    seen_prints.add(stripped)

                # ìœ íš¨í•œ Python ì½”ë“œë§Œ ì¶”ê°€
                if (('=' in stripped) or
                    stripped.startswith('print(') or
                    stripped.startswith('result') or
                    stripped.startswith('data') or
                    ('    ' in line and line.strip())):  # ë“¤ì—¬ì“°ê¸°ëœ ë¼ì¸
                    code_lines.append(line.rstrip())

            return '\n'.join(code_lines)

        # ì›ë³¸ ì½”ë“œ ë³´ê´€ (ë””ë²„ê¹…ìš©)
        original_code = generated_code
        generated_code = clean_code(generated_code)

        # ì •ì œëœ ì½”ë“œë¥¼ ì²­í¬ë¡œ ë¶„í• 
        code_chunks = self._split_code_into_chunks(generated_code)

        # ì½”ë“œ ì‹¤í–‰
        execution_result = await self._execute_analysis_code(generated_code, pd.DataFrame())

        # ê²°ê³¼ í¬ë§·íŒ…
        return {
            'output': execution_result.get('output', ''),
            'code_chunks': code_chunks,
            'result': execution_result.get('result', ''),
            'insights': [
                "âœ… ê³„ì‚°ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
            ],
            'success': execution_result.get('success', False)
        }

    async def _execute_analysis_code_with_data(self, code: str, df: pd.DataFrame) -> Dict[str, Any]:
        """ì‹¤ì œ ë°ì´í„°ì™€ í•¨ê»˜ ë¶„ì„ ì½”ë“œ ì‹¤í–‰"""
        try:
            # ì½”ë“œ ì‹¤í–‰ API í˜¸ì¶œ
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    "http://localhost:8000/api/code/execute",
                    json={
                        "code": code,
                        "context": {
                            "df": df.to_dict('list'),  # list í˜•íƒœë¡œ ì§ë ¬í™”
                            "data": df.to_dict('list'),
                            "rows": len(df),
                            "columns": df.columns.tolist()
                        }
                    },
                    timeout=30.0
                )

                if response.status_code == 200:
                    return response.json()
                else:
                    return {
                        "success": False,
                        "output": "",
                        "error": f"ì‹¤í–‰ ì‹¤íŒ¨: {response.status_code}",
                        "execution_time": 0
                    }

        except Exception as e:
            return {
                "success": False,
                "output": "",
                "error": f"ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}",
                "execution_time": 0
            }

    async def _generate_follow_up_questions(self, original_question: str, analysis_result: Dict[str, Any]) -> List[str]:
        """ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê´€ë ¨ ì§ˆë¬¸ ìƒì„±"""
        try:
            follow_up_prompt = f"""
ì‚¬ìš©ìžê°€ "{original_question}"ë¼ê³  ì§ˆë¬¸í–ˆê³ , ë‹¤ìŒê³¼ ê°™ì€ ë¶„ì„ ê²°ê³¼ë¥¼ ì–»ì—ˆìŠµë‹ˆë‹¤:

ê²°ê³¼: {analysis_result.get('output', '')[:500]}...

ì´ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìžê°€ ì¶”ê°€ë¡œ ê¶ê¸ˆí•´í•  ë§Œí•œ ê´€ë ¨ ì§ˆë¬¸ 3ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

ìš”êµ¬ì‚¬í•­:
1. í˜„ìž¬ ë¶„ì„ì„ ì‹¬í™”ì‹œí‚¬ ìˆ˜ ìžˆëŠ” ì§ˆë¬¸
2. ë‹¤ë¥¸ ê´€ì ì—ì„œ ì ‘ê·¼í•  ìˆ˜ ìžˆëŠ” ì§ˆë¬¸
3. ì‹¤ë¬´ì ìœ¼ë¡œ ìœ ìš©í•œ ì§ˆë¬¸
4. ê° ì§ˆë¬¸ì€ í•œ ë¬¸ìž¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ
5. ì§ˆë¬¸ë§Œ ë°˜í™˜ (ë²ˆí˜¸ë‚˜ ì„¤ëª… ì—†ì´)

ì˜ˆì‹œ:
ì´ ë°ì´í„°ì—ì„œ ì´ìƒì¹˜ëŠ” ì–´ë–»ê²Œ ë¶„í¬ë˜ì–´ ìžˆë‚˜ìš”?
ì‹œê°„ëŒ€ë³„ íŠ¸ë Œë“œëŠ” ì–´ë–»ê²Œ ë³€í™”í•˜ê³  ìžˆë‚˜ìš”?
ë‹¤ë¥¸ ë³€ìˆ˜ì™€ì˜ ìƒê´€ê´€ê³„ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?
"""

            response = await self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë°ì´í„° ë¶„ì„ ê´€ë ¨ í›„ì† ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” ì „ë¬¸ê°€ìž…ë‹ˆë‹¤."},
                    {"role": "user", "content": follow_up_prompt}
                ],
                temperature=0.7,
                max_tokens=200
            )

            questions_text = response.choices[0].message.content.strip()
            questions = [q.strip() for q in questions_text.split('\n') if q.strip()]

            return questions[:3]  # ìµœëŒ€ 3ê°œë§Œ ë°˜í™˜

        except Exception as e:
            print(f"Follow-up questions generation error: {str(e)}")
            return [
                "ì´ ê²°ê³¼ë¥¼ ë‹¤ë¥¸ ê´€ì ì—ì„œ ë¶„ì„í•´ë³¼ ìˆ˜ ìžˆì„ê¹Œìš”?",
                "ì¶”ê°€ì ìœ¼ë¡œ í™•ì¸í•´ë³¼ ë§Œí•œ íŒ¨í„´ì´ ìžˆì„ê¹Œìš”?",
                "ì´ ë°ì´í„°ì˜ ë‹¤ë¥¸ íŠ¹ì„±ì€ ì–´ë–»ê²Œ ë ê¹Œìš”?"
            ]

    # ========== ChatGPT ìŠ¤íƒ€ì¼ ìƒˆë¡œìš´ í•¨ìˆ˜ë“¤ ==========

    async def _generate_chatgpt_style_code(self, question: str, df: pd.DataFrame, file_info: dict) -> str:
        """ChatGPT ìŠ¤íƒ€ì¼ ë¹ ë¥¸ Python ì½”ë“œ ìƒì„± (ë°ì´í„° ê¸°ë°˜)"""
        try:
            # ë°ì´í„° ë¶„ì„ - ì»¬ëŸ¼ ì •ë³´ ìˆ˜ì§‘
            columns = df.columns.tolist()
            column_info = {}
            for col in columns[:10]:  # ìµœëŒ€ 10ê°œ ì»¬ëŸ¼ë§Œ ë¶„ì„
                try:
                    dtype = str(df[col].dtype)
                    unique_count = len(df[col].unique())
                    sample_values = df[col].dropna().head(3).tolist()
                    column_info[col] = {
                        'type': dtype,
                        'unique_count': unique_count,
                        'sample_values': sample_values
                    }
                except:
                    column_info[col] = {'type': 'unknown', 'unique_count': 0, 'sample_values': []}

            prompt = f"""
ë‹¹ì‹ ì€ ChatGPTì²˜ëŸ¼ ë°ì´í„° ë¶„ì„ Python ì½”ë“œë¥¼ ìƒì„±í•˜ëŠ” ì „ë¬¸ê°€ìž…ë‹ˆë‹¤.

ì§ˆë¬¸: {question}

**ì‹¤ì œ ë°ì´í„° ì •ë³´** (ì´ë¯¸ df ë³€ìˆ˜ì— ë¡œë“œë¨):
- ì´ {len(df)}í–‰, {len(df.columns)}ì—´
- ì‹¤ì œ ì»¬ëŸ¼ëª…: {columns}
- ì»¬ëŸ¼ë³„ ìƒì„¸ ì •ë³´: {column_info}

**ì ˆëŒ€ ê¸ˆì§€ì‚¬í•­**:
- ìƒˆë¡œìš´ DataFrame ìƒì„± ê¸ˆì§€ (dfëŠ” ì´ë¯¸ ì¡´ìž¬í•¨)
- ì˜ˆì‹œ ë°ì´í„° ìƒì„± ê¸ˆì§€
- data = {{}} ê°™ì€ ìƒˆ ë°ì´í„° ìƒì„± ê¸ˆì§€
- pd.DataFrame(data) ê°™ì€ ì½”ë“œ ê¸ˆì§€

**í•„ìˆ˜ ìš”êµ¬ì‚¬í•­**:
1. **ë°˜ë“œì‹œ ê¸°ì¡´ df ë³€ìˆ˜ ì‚¬ìš©** - ìƒˆë¡œ ë§Œë“¤ì§€ ë§ê³  ì´ë¯¸ ìžˆëŠ” df ì‚¬ìš©!
2. **ì ˆëŒ€ í•„ìˆ˜**: fig ë³€ìˆ˜ì— plotly ì°¨íŠ¸ ì €ìž¥
3. ì‹¤ì œ ì»¬ëŸ¼ëª…ì„ ì •í™•ížˆ ì‚¬ìš©
4. í•œêµ­ì–´ ì£¼ì„ê³¼ ì„¤ëª… í¬í•¨

**ì˜¬ë°”ë¥¸ ì½”ë“œ íŒ¨í„´**:
```python
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go

# ì‹¤ì œ ë°ì´í„° í™•ì¸ (dfëŠ” ì´ë¯¸ ë¡œë“œë˜ì–´ ìžˆìŒ)
print("=== ì‹¤ì œ ë°ì´í„° ë¶„ì„ ê²°ê³¼ ===")
print(f"ë°ì´í„° shape: {{df.shape}}")
print(f"ì»¬ëŸ¼: {{df.columns.tolist()}}")

# ì‹¤ì œ ë°ì´í„°ë¡œ ë¶„ì„
# df.describe(), df.info() ë“±ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ë°ì´í„° ë¶„ì„

# ì‹¤ì œ ì»¬ëŸ¼ëª…ì„ ì‚¬ìš©í•œ Plotly ì°¨íŠ¸ ìƒì„±
fig = px.bar(df, x='ì‹¤ì œì»¬ëŸ¼ëª…', title='ì‹¤ì œ ë°ì´í„° ë¶„ì„ ê²°ê³¼')

print("ðŸ“Š ì‹¤ì œ ë°ì´í„°ë¡œ ì°¨íŠ¸ ìƒì„± ì™„ë£Œ!")
```

**ìž˜ëª»ëœ ì˜ˆì‹œ (ì ˆëŒ€ ê¸ˆì§€)**:
```python
# ì´ëŸ° ì½”ë“œëŠ” ì ˆëŒ€ ìž‘ì„±í•˜ì§€ ë§ˆì„¸ìš”!
data = {{'col1': [1,2,3], 'col2': [4,5,6]}}  # ê¸ˆì§€!
df = pd.DataFrame(data)  # ê¸ˆì§€!
```

ì‹¤ì œ ë°ì´í„°(df)ë¥¼ ì‚¬ìš©í•œ ì™„ì „í•œ ì‹¤í–‰ ê°€ëŠ¥í•œ Python ì½”ë“œë§Œ ë°˜í™˜í•˜ì„¸ìš”:
"""

            response = await self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ChatGPTì™€ ê°™ì€ ë°ì´í„° ë¶„ì„ Python ì½”ë“œ ìƒì„± ì „ë¬¸ê°€ìž…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=1500
            )

            generated_code = response.choices[0].message.content.strip()

            # ì½”ë“œ ë¸”ë¡ ë§ˆì»¤ ì œê±°
            if "```python" in generated_code:
                generated_code = generated_code.split("```python")[1].split("```")[0]
            elif "```" in generated_code:
                generated_code = generated_code.split("```")[1].split("```")[0]

            return generated_code.strip()

        except Exception as e:
            print(f"ChatGPT ìŠ¤íƒ€ì¼ ì½”ë“œ ìƒì„± ì˜¤ë¥˜: {e}")
            return f"""
import pandas as pd
import plotly.express as px

print("=== ë°ì´í„° ë¶„ì„ ===")
print(f"ë°ì´í„° shape: {{df.shape}}")
print(f"ì»¬ëŸ¼: {{df.columns.tolist()}}")

# ê¸°ë³¸ ì°¨íŠ¸ ìƒì„±
if len(df.columns) >= 2:
    fig = px.scatter(df, x=df.columns[0], y=df.columns[1], title="ë°ì´í„° ë¶„ì„ ê²°ê³¼")
else:
    fig = px.bar(x=['ë°ì´í„°'], y=[len(df)], title="ë°ì´í„° ê°œìˆ˜")

print("ðŸ“Š ê¸°ë³¸ ì°¨íŠ¸ ìƒì„± ì™„ë£Œ!")
"""

    async def _needs_python_code(self, question: str) -> bool:
        """ì§ˆë¬¸ì´ Python ì½”ë“œ ì‹¤í–‰ì„ í•„ìš”ë¡œ í•˜ëŠ”ì§€ íŒë‹¨"""
        try:
            prompt = f"""
ë‹¤ìŒ ì§ˆë¬¸ì´ Python ì½”ë“œ ì‹¤í–‰ì´ í•„ìš”í•œì§€ íŒë‹¨í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

Python ì½”ë“œê°€ í•„ìš”í•œ ê²½ìš°:
- ìˆ˜í•™ì  ê³„ì‚° (ë³µìž¡í•œ ì—°ì‚°, í†µê³„ ê³„ì‚°)
- ë°ì´í„° ì‹œê°í™”ê°€ í•„ìš”í•œ ê²½ìš°
- ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„ì´ë‚˜ ì‹œë®¬ë ˆì´ì…˜
- ìˆ˜ì¹˜ ë¶„ì„ì´ë‚˜ ê·¸ëž˜í”„ ìƒì„±

Python ì½”ë“œê°€ ë¶ˆí•„ìš”í•œ ê²½ìš°:
- ì¼ë°˜ì ì¸ ì§ˆë¬¸ê³¼ ë‹µë³€
- ê°œë… ì„¤ëª…ì´ë‚˜ ì •ì˜
- ê°„ë‹¨í•œ ì‚¬ì‹¤ í™•ì¸
- ì¡°ì–¸ì´ë‚˜ ì˜ê²¬ ìš”ì²­

'YES' ë˜ëŠ” 'NO'ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.
"""

            response = await self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì§ˆë¬¸ ë¶„ì„ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. Python ì½”ë“œ ì‹¤í–‰ì´ í•„ìš”í•œì§€ ì •í™•ížˆ íŒë‹¨í•˜ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=10
            )

            answer = response.choices[0].message.content.strip().upper()
            return answer == "YES"

        except Exception as e:
            print(f"ì½”ë“œ í•„ìš”ì„± íŒë‹¨ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ì ìœ¼ë¡œ ì½”ë“œ ìƒì„±í•˜ì§€ ì•ŠìŒ
            return False

    async def _generate_chatgpt_general_code(self, question: str) -> str:
        """ChatGPT ìŠ¤íƒ€ì¼ ì¼ë°˜ ê³„ì‚°/ë¶„ì„ ì½”ë“œ ìƒì„± (íŒŒì¼ ì—†ëŠ” ê²½ìš°)"""
        try:
            prompt = f"""
ë‹¹ì‹ ì€ ChatGPTì²˜ëŸ¼ Python ì½”ë“œë¥¼ ìƒì„±í•˜ëŠ” ì „ë¬¸ê°€ìž…ë‹ˆë‹¤.

ì§ˆë¬¸: {question}

ìš”êµ¬ì‚¬í•­:
1. ì§ˆë¬¸ì— ë§žëŠ” Python ê³„ì‚°/ë¶„ì„ ì½”ë“œ ìž‘ì„±
2. numpy, pandas, plotly ë“± í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
3. ê°€ëŠ¥í•˜ë©´ ì‹œê°í™” í¬í•¨ (fig ë³€ìˆ˜ì— ì €ìž¥)
4. ì‹¤í–‰ ê°€ëŠ¥í•œ ì™„ì „í•œ ì½”ë“œë§Œ ë°˜í™˜
5. í•œêµ­ì–´ ì£¼ì„ê³¼ ì„¤ëª… í¬í•¨

ì™„ì „í•œ ì‹¤í–‰ ê°€ëŠ¥í•œ Python ì½”ë“œë§Œ ë°˜í™˜í•˜ì„¸ìš”:
"""

            response = await self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ChatGPTì™€ ê°™ì€ Python ì½”ë“œ ìƒì„± ì „ë¬¸ê°€ìž…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=1000,
                stream=True  # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
            )

            generated_code = ""
            async for chunk in response:
                if chunk.choices[0].delta.content:
                    generated_code += chunk.choices[0].delta.content

            # ì½”ë“œ ë¸”ë¡ ë§ˆì»¤ ì œê±°
            if "```python" in generated_code:
                generated_code = generated_code.split("```python")[1].split("```")[0]
            elif "```" in generated_code:
                generated_code = generated_code.split("```")[1].split("```")[0]

            return generated_code.strip()

        except Exception as e:
            print(f"ì¼ë°˜ ì½”ë“œ ìƒì„± ì˜¤ë¥˜: {e}")
            return f"""
import numpy as np
import matplotlib.pyplot as plt
import plotly.express as px

print("=== ê³„ì‚° ê²°ê³¼ ===")
print("ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ê³„ì‚° ì¤‘ìž…ë‹ˆë‹¤...")

# ê¸°ë³¸ ì°¨íŠ¸ ìƒì„±
fig = px.bar(x=['ê²°ê³¼'], y=[1], title="ê³„ì‚° ì™„ë£Œ")
print("ðŸ“Š ê³„ì‚° ì™„ë£Œ!")
"""

    def _execute_code_safely(self, code: str, globals_dict: dict) -> dict:
        """ì•ˆì „í•œ ì½”ë“œ ì‹¤í–‰ with ì¶œë ¥ ìº¡ì²˜"""
        try:
            # ì¶œë ¥ ìº¡ì²˜ë¥¼ ìœ„í•œ StringIO
            output_buffer = io.StringIO()
            error_buffer = io.StringIO()

            # stdout, stderr ë¦¬ë””ë ‰ì…˜
            with redirect_stdout(output_buffer), redirect_stderr(error_buffer):
                exec(code, globals_dict)

            output = output_buffer.getvalue()
            error = error_buffer.getvalue()

            # ì¶”ê°€ë¡œ ê²°ê³¼ê°’ ì¶”ì¶œ (printê°€ ì—†ì–´ë„ ê²°ê³¼ í™•ë³´)
            additional_results = []
            for key, value in globals_dict.items():
                if key not in ['pd', 'np', 'plt', 'px', 'go', 'pio', 'json', 'plotly', 'df', '__builtins__']:
                    try:
                        # ìˆ«ìžë‚˜ ê°„ë‹¨í•œ ê°’ë“¤ë§Œ ê²°ê³¼ì— í¬í•¨
                        if isinstance(value, (int, float, str, bool, list, tuple)) and len(str(value)) < 500:
                            additional_results.append(f"{key}: {value}")
                        elif hasattr(value, 'shape') and hasattr(value, 'dtype'):  # numpy arrayë‚˜ pandas Series
                            additional_results.append(f"{key}: {type(value).__name__} shape={value.shape}")
                    except:
                        pass

            # ì¶œë ¥ ê²°í•©
            if additional_results:
                if output:
                    output += "\n" + "\n".join(additional_results)
                else:
                    output = "\n".join(additional_results)

            return {
                'success': True,
                'output': output,
                'error': error if error else None
            }

        except Exception as e:
            import traceback
            return {
                'success': False,
                'output': '',
                'error': f"{str(e)}\n{traceback.format_exc()}"
            }

    async def _execute_code_via_api(self, code: str, df: pd.DataFrame) -> dict:
        """ðŸš€ í”„ë¡ íŠ¸ì—”ë“œ ì„±ê³µ íŒŒì´í”„ë¼ì¸ê³¼ ì™„ì „ížˆ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì‹¤í–‰"""
        try:
            import httpx
            import json
            import numpy as np

            print(f"ðŸ”¥ í”„ë¡ íŠ¸ì—”ë“œ ì„±ê³µ íŒŒì´í”„ë¼ì¸ ì‚¬ìš© - ì½”ë“œ ê¸¸ì´: {len(code)}")

            # ë°ì´í„°í”„ë ˆìž„ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (í”„ë¡ íŠ¸ì—”ë“œì™€ 100% ë™ì¼)
            context = {}
            if df is not None and not df.empty:
                # ë‹¨ê³„ë³„ ê°•ë ¥í•œ NaN ì²˜ë¦¬
                df_clean = df.copy()

                # 1. ëª¨ë“  NaN, inf ê°’ì„ Noneìœ¼ë¡œ ë³€í™˜
                df_clean = df_clean.replace([np.nan, np.inf, -np.inf], None)

                # 2. Noneì„ ìˆ«ìž ì»¬ëŸ¼ì—ì„œëŠ” 0ìœ¼ë¡œ, ë¬¸ìžì—´ ì»¬ëŸ¼ì—ì„œëŠ” ë¹ˆ ë¬¸ìžì—´ë¡œ ë³€í™˜
                for col in df_clean.columns:
                    if df_clean[col].dtype in ['float64', 'int64', 'float32', 'int32']:
                        df_clean[col] = df_clean[col].fillna(0)
                    else:
                        df_clean[col] = df_clean[col].fillna('')

                # 3. to_dict ë³€í™˜
                df_dict = df_clean.to_dict('list')

                # 4. ë”•ì…”ë„ˆë¦¬ ë‚´ ëª¨ë“  ê°’ì„ JSON ì•ˆì „ íƒ€ìž…ìœ¼ë¡œ ë³€í™˜
                def clean_value(val):
                    if pd.isna(val) or val is None:
                        return None
                    elif isinstance(val, (np.integer, int)):
                        return int(val)
                    elif isinstance(val, (np.floating, float)):
                        if np.isnan(val) or np.isinf(val):
                            return 0
                        return float(val)
                    elif isinstance(val, str):
                        return str(val)
                    else:
                        return str(val)

                # 5. ëª¨ë“  ê°’ì„ ìž¬ê·€ì ìœ¼ë¡œ ì •ë¦¬
                cleaned_dict = {}
                for key, values in df_dict.items():
                    cleaned_dict[key] = [clean_value(v) for v in values]

                context['df'] = cleaned_dict
                print(f"ðŸ“Š ë°ì´í„° ë³€í™˜ ì™„ë£Œ: {len(df)}í–‰, {len(df.columns)}ì—´ (ê°•ë ¥í•œ NaN ì²˜ë¦¬ ì ìš©)")

            # ìš”ì²­ ë°ì´í„° êµ¬ì„± (í”„ë¡ íŠ¸ì—”ë“œì™€ ë™ì¼)
            request_data = {
                "code": code,
                "context": context
            }

            # JSON ì§ë ¬í™” í…ŒìŠ¤íŠ¸
            try:
                json.dumps(request_data, ensure_ascii=False)
                print(f"âœ… JSON ì§ë ¬í™” í…ŒìŠ¤íŠ¸ í†µê³¼")
            except Exception as json_error:
                print(f"âŒ JSON ì§ë ¬í™” í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {json_error}")
                return {
                    "success": False,
                    "output": "",
                    "error": f"ë°ì´í„° ì§ë ¬í™” ì˜¤ë¥˜: {str(json_error)}"
                }

            print(f"ðŸŒ API í˜¸ì¶œ ì‹œìž‘: /api/code/execute")

            # ë‚´ë¶€ API í˜¸ì¶œ (í”„ë¡ íŠ¸ì—”ë“œì™€ ë™ì¼í•œ ì—”ë“œí¬ì¸íŠ¸)
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    "http://localhost:8001/api/code/execute",
                    json=request_data,
                    timeout=30.0
                )

                if response.status_code == 200:
                    result = response.json()
                    success = result.get('success', False)
                    chart_data = result.get('chart_data')
                    output = result.get('output', '')

                    print(f"âœ… í”„ë¡ íŠ¸ì—”ë“œ íŒŒì´í”„ë¼ì¸ ì„±ê³µ!")
                    print(f"  - ì„±ê³µ: {success}")
                    print(f"  - ì¶œë ¥ ê¸¸ì´: {len(output) if output else 0}")
                    print(f"  - ì°¨íŠ¸ ë°ì´í„°: {'ìžˆìŒ' if chart_data else 'ì—†ìŒ'}")

                    if chart_data:
                        print(f"ðŸŽ¨ ì°¨íŠ¸ ë°ì´í„° í¬ê¸°: {len(str(chart_data))}")

                    return {
                        'success': success,
                        'output': output,
                        'error': result.get('error'),
                        'chart_data': chart_data,
                        'execution_time': result.get('execution_time', 0)
                    }
                else:
                    print(f"âŒ API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
                    return {
                        'success': False,
                        'output': '',
                        'error': f"API í˜¸ì¶œ ì‹¤íŒ¨: HTTP {response.status_code}"
                    }

        except Exception as e:
            print(f"âŒ API ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            import traceback
            print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return {
                'success': False,
                'output': '',
                'error': f"API ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}"
            }

    def _analyze_generated_code_for_insights(self, code: str, question: str, df: pd.DataFrame) -> str:
        """ðŸ§  ìƒì„±ëœ ì½”ë“œë¥¼ ë¶„ì„í•´ì„œ ìŠ¤ë§ˆíŠ¸í•œ ì¸ì‚¬ì´íŠ¸ ì œê³µ"""
        try:
            print(f"ðŸ” ì½”ë“œ ë¶„ì„ ì‹œìž‘: {len(code)}ìž")

            # ë°ì´í„° ì •ë³´ ì¶”ì¶œ
            data_info = ""
            if df is not None and not df.empty:
                data_info = f"ë°ì´í„°ì…‹: {len(df)}í–‰ {len(df.columns)}ì—´\n"
                data_info += f"ì»¬ëŸ¼: {', '.join(df.columns.tolist())}\n"

                # ìˆ«ìž ì»¬ëŸ¼ ì •ë³´
                numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
                if numeric_cols:
                    data_info += f"ìˆ«ìž ì»¬ëŸ¼: {', '.join(numeric_cols)}\n"

            # ì½”ë“œì—ì„œ ì£¼ìš” ë¶„ì„ ë‚´ìš© ì¶”ì¶œ
            analysis_content = ""

            # ì½”ë“œì—ì„œ ì°¨íŠ¸ íƒ€ìž… ì¶”ì¶œ
            if "px.line" in code or "plt.plot" in code:
                analysis_content += "ðŸ“ˆ ì„ í˜• ì°¨íŠ¸ ë¶„ì„: ì‹œê°„ì— ë”°ë¥¸ ë°ì´í„° ë³€í™” ì¶”ì„¸ë¥¼ ì‹œê°í™”\n"
            elif "px.bar" in code or "plt.bar" in code:
                analysis_content += "ðŸ“Š ë§‰ëŒ€ ì°¨íŠ¸ ë¶„ì„: ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„° ë¹„êµë¥¼ ì‹œê°í™”\n"
            elif "px.scatter" in code or "plt.scatter" in code:
                analysis_content += "ðŸ”µ ì‚°ì ë„ ë¶„ì„: ë‘ ë³€ìˆ˜ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ì‹œê°í™”\n"

            # ì½”ë“œì—ì„œ ë¶„ì„ ë°©ë²• ì¶”ì¶œ
            if "mean()" in code or "í‰ê· " in code:
                analysis_content += "ðŸ’¡ í‰ê· ê°’ ê³„ì‚°ì„ í†µí•œ ì¤‘ì•™ê°’ ë¶„ì„\n"
            if "growth" in code or "ì„±ìž¥ë¥ " in code:
                analysis_content += "ðŸ“ˆ ì„±ìž¥ë¥  ë¶„ì„ì„ í†µí•œ ë³€í™” ì¶”ì„¸ íŒŒì•…\n"
            if "compare" in code or "ë¹„êµ" in code:
                analysis_content += "âš–ï¸ ë¹„êµ ë¶„ì„ì„ í†µí•œ ì°¨ì´ì  ì‹ë³„\n"

            # ì§ˆë¬¸ ê¸°ë°˜ ë§žì¶¤ ë¶„ì„
            question_insights = ""
            if "GDP" in question.upper():
                question_insights += "ðŸŒ GDP ë°ì´í„° ë¶„ì„: ê²½ì œ ì„±ìž¥ ë° êµ­ê°€ë³„ ë¹„êµ\n"
            if "growth" in question.lower() or "ì„±ìž¥" in question:
                question_insights += "ðŸ“Š ì„±ìž¥ë¥  ë¶„ì„: ì‹œê°„ì— ë”°ë¥¸ ë³€í™”ìœ¨ ê³„ì‚°\n"

            # ìµœì¢… ë¶„ì„ ê²°ê³¼ ì¡°í•©
            smart_analysis = f"""ðŸ¤– **ìŠ¤ë§ˆíŠ¸ ì½”ë“œ ë¶„ì„ ê²°ê³¼**

{data_info}
**ë¶„ì„ ë°©ë²•:**
{analysis_content}

**ì§ˆë¬¸ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸:**
{question_insights}

**ìƒì„±ëœ ë¶„ì„ ì½”ë“œ:**
- ë°ì´í„° ì²˜ë¦¬ ë° ê³„ì‚° ë¡œì§ êµ¬í˜„
- ì‹œê°í™” ì°¨íŠ¸ ìƒì„± ({len(code)}ìžì˜ Python ì½”ë“œ)
- ë¶„ì„ ê²°ê³¼ë¥¼ ì°¨íŠ¸ë¡œ í‘œí˜„

**ì˜ˆìƒ ê²°ê³¼:**
ì´ ë¶„ì„ì„ í†µí•´ {question}ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ë‹µë³€ì„ ì–»ì„ ìˆ˜ ìžˆìœ¼ë©°,
ìƒì„±ëœ ì°¨íŠ¸ë¥¼ í†µí•´ ì‹œê°ì ìœ¼ë¡œ ë°ì´í„°ì˜ íŒ¨í„´ê³¼ íŠ¸ë Œë“œë¥¼ í™•ì¸í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
"""

            print(f"âœ… ìŠ¤ë§ˆíŠ¸ ë¶„ì„ ì™„ë£Œ: {len(smart_analysis)}ìž")
            return smart_analysis

        except Exception as e:
            print(f"âŒ ì½”ë“œ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return f"ìƒì„±ëœ ë¶„ì„ ì½”ë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ {question}ì— ëŒ€í•œ ë¶„ì„ì´ ìˆ˜í–‰ë˜ì—ˆìŠµë‹ˆë‹¤. ì°¨íŠ¸ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”."

    async def _generate_chatgpt_insights(self, question: str, execution_result: str,
                                       chart_data: dict, df: pd.DataFrame) -> list:
        """ChatGPT ìŠ¤íƒ€ì¼ ê³ í’ˆì§ˆ AI ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        try:
            # ì‹¤í–‰ ê²°ê³¼ì—ì„œ ì‹¤ì œ ìˆ˜ì¹˜ ë°ì´í„° ì¶”ì¶œ
            actual_numbers = []
            if execution_result:
                import re
                # ìˆ«ìž íŒ¨í„´ ì°¾ê¸° (ê°œìˆ˜, ë¹„ìœ¨ ë“±)
                numbers = re.findall(r'\d+[,.]?\d*', execution_result)
                actual_numbers = numbers[:10]  # ìµœëŒ€ 10ê°œ

            # ë°ì´í„° ì •ë³´ ìš”ì•½
            data_summary = ""
            if df is not None and not df.empty:
                data_summary = f"ì‹¤ì œ ë°ì´í„°: {len(df):,}ê±´ì˜ ë ˆì½”ë“œ, {len(df.columns)}ê°œ ì»¬ëŸ¼"

            prompt = f"""
ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ë¡œì„œ ì‹¤ì œ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

**ë¶„ì„ ìƒí™©:**
ì§ˆë¬¸: {question}
{data_summary}
ì‹¤í–‰ ê²°ê³¼: {execution_result}
ì¶”ì¶œëœ ìˆ˜ì¹˜: {actual_numbers}
ì°¨íŠ¸: {'ìƒì„±ë¨' if chart_data else 'ì—†ìŒ'}

**ìš”êµ¬ì‚¬í•­:**
1. **ì‹¤ì œ ë°ì´í„° ê²°ê³¼ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì–¸ê¸‰** (ì¼ë°˜ë¡  ê¸ˆì§€)
2. **ì‹¤í–‰ ê²°ê³¼ì˜ ìˆ«ìžë¥¼ í™œìš©í•œ êµ¬ì²´ì  ë¶„ì„**
3. **ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ êµ¬ì¡°í™”ëœ ì¸ì‚¬ì´íŠ¸**
4. **ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ì ì˜ ì‹¤ìš©ì  í•´ì„**

**ì¸ì‚¬ì´íŠ¸ í˜•ì‹ (ë‹¤ì–‘í•˜ê²Œ í™œìš©):**
- **ì œëª© í™œìš©**: ## ì£¼ìš” ë°œê²¬, ### í•µì‹¬ í¬ì¸íŠ¸
- **ê°•ì¡°**: **ì¤‘ìš”í•œ ìˆ˜ì¹˜**, *ì£¼ëª©í•  ì *
- **ë¦¬ìŠ¤íŠ¸**: â€¢ êµ¬ì²´ì  ì‚¬ì‹¤ë“¤
- **ì¸ìš©**: > í•µì‹¬ ê²°ë¡ 

**ì ˆëŒ€ ê¸ˆì§€:**
- "íŠ¹ì • ë„¤íŠ¸ì›Œí¬ì— ì§‘ì¤‘ë˜ì–´ ìžˆìœ¼ë©°..." ê°™ì€ ëª¨í˜¸í•œ í‘œí˜„
- "ì†Œë¹„ìž ì„ í˜¸ë„ë¥¼ ë°˜ì˜í•©ë‹ˆë‹¤" ê°™ì€ ë»”í•œ í•´ì„
- ì‹¤ì œ ë°ì´í„° ê²°ê³¼ì™€ ë¬´ê´€í•œ ì¼ë°˜ë¡ 

ì‹¤ì œ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ êµ¬ì²´ì ì´ê³  ìœ ìš©í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ìž‘ì„±í•˜ì„¸ìš”:
"""

            response = await self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì‹¤ì œ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ëŠ” ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=800
            )

            insights_text = response.choices[0].message.content.strip()

            # ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì„ ê³ ë ¤í•œ ì¸ì‚¬ì´íŠ¸ ë°˜í™˜
            return [insights_text]  # í•˜ë‚˜ì˜ ì™„ì„±ëœ ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜

        except Exception as e:
            print(f"ì¸ì‚¬ì´íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")
            return [
                "## ë¶„ì„ ì™„ë£Œ\n\nì‹¤ì œ ë°ì´í„° ë¶„ì„ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ìƒì„±ëœ ì°¨íŠ¸ë¥¼ í†µí•´ ë°ì´í„°ì˜ ë¶„í¬ì™€ íŒ¨í„´ì„ í™•ì¸í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤."
            ]

    async def _generate_comprehensive_answer(self, question: str, execution_result: str,
                                           insights: list, chart_data: dict) -> str:
        """ChatGPT ìŠ¤íƒ€ì¼ ê³ í’ˆì§ˆ ì¢…í•© ë‹µë³€ ìƒì„±"""
        try:
            # ì‹¤í–‰ ê²°ê³¼ì—ì„œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ
            key_findings = []
            if execution_result:
                lines = execution_result.split('\n')
                for line in lines:
                    if any(keyword in line.lower() for keyword in ['count', 'ê°œìˆ˜', 'ë¹„ìœ¨', 'í‰ê· ', 'ìµœëŒ€', 'ìµœì†Œ', 'í•©ê³„']):
                        key_findings.append(line.strip())

            prompt = f"""
ë‹¹ì‹ ì€ ChatGPTì²˜ëŸ¼ ë°ì´í„° ë¶„ì„ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  ì „ë¬¸ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ëŠ” AIìž…ë‹ˆë‹¤.

**ë¶„ì„ ìš”ì²­:** {question}

**ì‹¤í–‰ ê²°ê³¼:**
{execution_result}

**í•µì‹¬ ë°œê²¬ì‚¬í•­:**
{key_findings}

**ì°¨íŠ¸:** {'âœ… ìƒì„±ë¨' if chart_data else 'âŒ ì—†ìŒ'}

**ë‹µë³€ ìš”êµ¬ì‚¬í•­:**
1. **ì§ˆë¬¸ì— ëŒ€í•œ ëª…í™•í•œ ë‹µë³€**ìœ¼ë¡œ ì‹œìž‘
2. **ì‹¤ì œ ìˆ˜ì¹˜ì™€ ê²°ê³¼ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì–¸ê¸‰**
3. **ê°„ê²°í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ì„¤ëª…**
4. ë¶ˆí•„ìš”í•œ ìž¥í™©í•œ ì„¤ëª… ì§€ì–‘

ë‹¤ìŒê³¼ ê°™ì€ ChatGPT ìŠ¤íƒ€ì¼ë¡œ ë‹µë³€í•˜ì„¸ìš”:
- ì§ì ‘ì ì´ê³  ëª…í™•í•œ ë‹µë³€
- ì‹¤ì œ ë°ì´í„° ê²°ê³¼ ê¸°ë°˜
- ì „ë¬¸ì ì´ì§€ë§Œ ì´í•´í•˜ê¸° ì‰¬ìš´ í†¤

í•œêµ­ì–´ë¡œ ìžì—°ìŠ¤ëŸ½ê²Œ ìž‘ì„±í•˜ì„¸ìš”:
"""

            response = await self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ChatGPTì™€ ê°™ì´ ëª…í™•í•˜ê³  ì§ì ‘ì ì¸ ë°ì´í„° ë¶„ì„ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ì „ë¬¸ê°€ìž…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.4,
                max_tokens=600
            )

            return response.choices[0].message.content.strip()

        except Exception as e:
            print(f"ì¢…í•© ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")
            return f"""**{question}**ì— ëŒ€í•œ ë¶„ì„ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.

**ë¶„ì„ ê²°ê³¼:**
{execution_result.split('=== ì‹¤ì œ ë°ì´í„° ë¶„ì„ ê²°ê³¼ ===')[-1] if '=== ì‹¤ì œ ë°ì´í„° ë¶„ì„ ê²°ê³¼ ===' in execution_result else execution_result}

{'ðŸ“Š ìœ„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì°¨íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.' if chart_data else ''}
"""

    async def unified_analysis_stream(self, question: str, df: pd.DataFrame = None, file_info: dict = None, conversation_history: list = None):
        """
        ChatGPT ìŠ¤íƒ€ì¼ ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„ ì‹œìŠ¤í…œ:
        ì‹¤ì‹œê°„ìœ¼ë¡œ ë¶„ì„ ë‹¨ê³„ë³„ ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.
        """
        try:
            print(f"ðŸš€ ChatGPT ìŠ¤íƒ€ì¼ ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„ ì‹œìž‘: {question}")
            print(f"ðŸŽ¯ unified_analysis_stream í•¨ìˆ˜ ì‹¤í–‰ë¨ - generated_code ì¶”ì  ì‹œìž‘")

            # 1ë‹¨ê³„: ë¶„ì„ ì‹œìž‘ ì•Œë¦¼
            print(f"ðŸ“¤ 1ë‹¨ê³„: analysis_start yield ì‹œìž‘")
            try:
                yield {
                    "type": "analysis_start",
                    "content": "ë¶„ì„ì„ ì‹œìž‘í•©ë‹ˆë‹¤...",
                    "step": "preparing"
                }
                print(f"âœ… 1ë‹¨ê³„: analysis_start yield ì™„ë£Œ")
            except Exception as yield_error:
                print(f"âŒ 1ë‹¨ê³„ yield ì˜¤ë¥˜: {str(yield_error)}")
                raise yield_error

            await asyncio.sleep(0.1)  # ìž‘ì€ ì§€ì—° ì¶”ê°€

            # 2ë‹¨ê³„: ëŒ€í™” ížˆìŠ¤í† ë¦¬ ê°•í™”
            print(f"ðŸ”„ Enhanced question ìƒì„± ì‹œìž‘")
            try:
                # ëŒ€í™” ížˆìŠ¤í† ë¦¬ë¥¼ í™œìš©í•œ ì§ˆë¬¸ ê°•í™”
                enhanced_question = self._enhance_question_with_context(question, conversation_history)
                print(f"ðŸ”„ Enhanced question: {enhanced_question}")
            except Exception as enhance_error:
                print(f"âŒ Enhanced question ìƒì„± ì˜¤ë¥˜: {str(enhance_error)}")
                enhanced_question = question

            print(f"ðŸ“¤ 2ë‹¨ê³„: step_update yield ì‹œìž‘")
            try:
                yield {
                    "type": "step_update",
                    "content": "ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ìžˆìŠµë‹ˆë‹¤...",
                    "step": "analyzing_question"
                }
                print(f"âœ… 2ë‹¨ê³„: step_update yield ì™„ë£Œ")
            except Exception as yield2_error:
                print(f"âŒ 2ë‹¨ê³„ yield ì˜¤ë¥˜: {str(yield2_error)}")
                raise yield2_error

            # 3ë‹¨ê³„: ì½”ë“œ í•„ìš”ì„± íŒë‹¨ ë° ìƒì„±
            if df is not None and not df.empty:
                print(f"ðŸ“Š ë°ì´í„° ê¸°ë°˜ ë¶„ì„ (í–‰: {len(df)}, ì—´: {len(df.columns)})")

                # ì‹¤ì‹œê°„ ì½”ë“œ ìƒì„± (ë¹ˆ ì½”ë“œë°•ìŠ¤ ì—†ì´ ë°”ë¡œ ì½”ë“œ ìƒì„±)
                generated_code = ""
                code_lines = []
                async for code_chunk in self._stream_code_generation(enhanced_question, df, file_info):
                    if code_chunk["type"] == "code_line":
                        code_lines.append(code_chunk["content"])
                        generated_code += code_chunk["content"] + "\n"
                    elif code_chunk["type"] == "code_complete":
                        generated_code = code_chunk["full_code"]
            else:
                # ì½”ë“œê°€ í•„ìš”í•œì§€ ë¨¼ì € íŒë‹¨
                needs_code = await self._needs_python_code(enhanced_question)
                if needs_code:
                    print("ðŸ”¢ Python ì½”ë“œ ê³„ì‚° í•„ìš”")

                    # ì‹¤ì‹œê°„ ì½”ë“œ ìƒì„± (ë¹ˆ ì½”ë“œë°•ìŠ¤ ì—†ì´)
                    generated_code = ""
                    code_lines = []
                    async for code_chunk in self._stream_code_generation(enhanced_question, df, file_info):
                        if code_chunk["type"] == "code_line":
                            code_lines.append(code_chunk["content"])
                            generated_code += code_chunk["content"] + "\n"
                        elif code_chunk["type"] == "code_complete":
                            generated_code = code_chunk["full_code"]
                else:
                    print("ðŸ’¬ ì¼ë°˜ í…ìŠ¤íŠ¸ ì‘ë‹µ ëª¨ë“œ")
                    generated_code = None

            if generated_code:
                print(f"âœ… Python ì½”ë“œ ìƒì„± ì™„ë£Œ")
            else:
                print(f"ðŸ’¬ í…ìŠ¤íŠ¸ ì‘ë‹µ ëª¨ë“œ")

            # 4ë‹¨ê³„: ì½”ë“œ ì‹¤í–‰ (ì½”ë“œê°€ ìžˆëŠ” ê²½ìš°ë§Œ)
            if generated_code:
                # ì™„ì„±ëœ ì½”ë“œë°•ìŠ¤ë¥¼ í•œ ë²ˆì— í‘œì‹œ
                yield {
                    "type": "code_complete_display",
                    "content": "ì½”ë“œ ì‹¤í–‰ ì¤‘...",
                    "code": generated_code,
                    "step": "displaying_code"
                }

                # ðŸš€ í”„ë¡ íŠ¸ì—”ë“œì™€ ë™ì¼í•œ ì½”ë“œ ì‹¤í–‰ íŒŒì´í”„ë¼ì¸ ì‚¬ìš©
                print(f"âš¡ ì½”ë“œ ì‹¤í–‰ ì‹œìž‘ (í”„ë¡ íŠ¸ì—”ë“œ íŒŒì´í”„ë¼ì¸ ì‚¬ìš©)...")
                print(f"ðŸ” ìƒì„±ëœ ì½”ë“œ:")
                print("=" * 50)
                print(generated_code)
                print("=" * 50)
                exec_result = await self._execute_code_via_api(generated_code, df)
                execution_result = exec_result.get('output', '')
                chart_data = exec_result.get('chart_data')

                # ë””ë²„ê¹…: ì½”ë“œ ì‹¤í–‰ ê²°ê³¼ ìƒì„¸ ë¡œê·¸
                print(f"ðŸ” ì½”ë“œ ì‹¤í–‰ ì™„ë£Œ (API ì‚¬ìš©):")
                print(f"  - ì„±ê³µ: {exec_result.get('success', False)}")
                print(f"  - ì¶œë ¥ ê¸¸ì´: {len(execution_result)}")
                print(f"  - ì¶œë ¥ ë‚´ìš©: {execution_result[:200]}..." if execution_result else "  - ì¶œë ¥ ì—†ìŒ")
                print(f"  - ì°¨íŠ¸ ë°ì´í„° ì¡´ìž¬: {bool(chart_data)}")
                if exec_result.get('error'):
                    print(f"  - ì˜¤ë¥˜: {exec_result.get('error')}")

                # ì‹¤í–‰ ê²°ê³¼ ìŠ¤íŠ¸ë¦¬ë°
                yield {
                    "type": "code_execution_result",
                    "content": execution_result,
                    "step": "code_executed"
                }

                # ì°¨íŠ¸ ë°ì´í„°ê°€ ìžˆìœ¼ë©´ ìŠ¤íŠ¸ë¦¬ë°
                if chart_data:
                    print(f"ðŸ“ˆ ì°¨íŠ¸ ë°ì´í„° ìˆ˜ì‹  ì„±ê³µ!")
                    yield {
                        "type": "chart_generated",
                        "content": "ì°¨íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.",
                        "chartData": chart_data,
                        "step": "chart_ready"
                    }

            # ì¸ì‚¬ì´íŠ¸ ìƒì„± ë¹„í™œì„±í™” (text_stream ë‹µë³€ìœ¼ë¡œ í†µí•©)
            insights = []
            follow_up_questions = []

            # í›„ì† ì§ˆë¬¸ë§Œ ìƒì„± (ì¸ì‚¬ì´íŠ¸ëŠ” í…ìŠ¤íŠ¸ ë‹µë³€ì— í¬í•¨)
            if generated_code:
                print(f"ðŸ”„ í›„ì† ì§ˆë¬¸ ìƒì„± ì‹œìž‘...")
                follow_up_questions = await self._generate_follow_up_questions(question, {
                    'output': execution_result,
                    'chart_data': chart_data
                })
                print(f"âœ… í›„ì† ì§ˆë¬¸ ìƒì„± ì™„ë£Œ: {len(follow_up_questions)}ê°œ")

            # 7ë‹¨ê³„: ìµœì¢… ë‹µë³€ ìƒì„± (ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°)
            print(f"ðŸ” generated_code ì²´í¬: {bool(generated_code)}, ê¸¸ì´: {len(generated_code) if generated_code else 0}")

            if generated_code:
                # ðŸŽ¯ **í˜ì‹ ì  í•´ê²°ì±…**: ì°¨íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨ë¥¼ ë³´ì™„í•˜ëŠ” ìŠ¤ë§ˆíŠ¸ ë¶„ì„
                print(f"ðŸ’¡ ìŠ¤ë§ˆíŠ¸ ë¶„ì„ ëª¨ë“œ í™œì„±í™” - ì°¨íŠ¸ ì‹¤í–‰ ë¬´ê´€í•˜ê²Œ ì˜ë¯¸ìžˆëŠ” ë¶„ì„ ì œê³µ")

                # ìƒì„±ëœ ì½”ë“œ ìžì²´ë¥¼ ë¶„ì„í•´ì„œ ì¸ì‚¬ì´íŠ¸ ì œê³µ
                smart_analysis = self._analyze_generated_code_for_insights(generated_code, enhanced_question, df)

                # ì‹¤í–‰ ê²°ê³¼ê°€ ì—†ì–´ë„ ì½”ë“œ ê¸°ë°˜ ë‹µë³€ ìƒì„±
                if not execution_result:
                    execution_result = smart_analysis

                print(f"ðŸŽ¯ ìŠ¤ë§ˆíŠ¸ ë¶„ì„ ê¸°ë°˜ ë‹µë³€ ìƒì„± ì‹œìž‘ - ë¶„ì„ ê¸¸ì´: {len(execution_result)}ìž")
                print(f"ðŸŽ¯ chart_data ì¡´ìž¬ ì—¬ë¶€: {bool(chart_data)}")

                comprehensive_answer = ""
                try:
                    async for answer_chunk in self._stream_comprehensive_answer(
                        enhanced_question,  # ëŒ€í™” ë§¥ë½ì´ í¬í•¨ëœ ì§ˆë¬¸ ì‚¬ìš©
                        execution_result,
                        insights,
                        chart_data
                    ):
                        print(f"ðŸŽ¯ ë‹µë³€ ì²­í¬ ë°›ìŒ: {answer_chunk[:20]}...")
                        yield {
                            "type": "text_stream",
                            "content": answer_chunk,
                            "step": "streaming_answer"
                        }
                        comprehensive_answer += answer_chunk
                    print(f"ðŸŽ¯ ìŠ¤ë§ˆíŠ¸ ë¶„ì„ ê¸°ë°˜ ë‹µë³€ ì™„ë£Œ - ì´ ê¸¸ì´: {len(comprehensive_answer)}")
                except Exception as text_error:
                    print(f"âŒ í…ìŠ¤íŠ¸ ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {text_error}")
                    # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ë‹µë³€ ì œê³µ
                    fallback_answer = "ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ìƒì„±ëœ ì°¨íŠ¸ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”."
                    yield {
                        "type": "text_stream",
                        "content": fallback_answer,
                        "step": "streaming_answer"
                    }
                    comprehensive_answer = fallback_answer
            else:
                # ì½”ë“œ ì—†ì´ ë°”ë¡œ í…ìŠ¤íŠ¸ ë‹µë³€ ìƒì„± (ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°)
                comprehensive_answer = ""
                async for answer_chunk in self._stream_simple_text_answer(question):
                    yield {
                        "type": "text_stream",
                        "content": answer_chunk,
                        "step": "streaming_text_answer"
                    }
                    comprehensive_answer += answer_chunk

            # 8ë‹¨ê³„: ì™„ë£Œëœ ì‘ë‹µ ì „ì†¡ (ë”ë¯¸ ë‹µë³€ ë°©ì§€)
            final_result = {
                "type": "analysis_complete",
                "answer": "",  # ë¹ˆ ê°’ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ë”ë¯¸ ë‹µë³€ ë°©ì§€
                "codeExecution": {
                    "codeChunks": [generated_code] if generated_code else [],
                    "isExecuting": False,
                    "result": execution_result if generated_code else "",
                    "output": execution_result if generated_code else ""
                },
                "insights": insights,
                "followUpQuestions": follow_up_questions,
                "chartData": chart_data,
                "step": "complete"
            }

            yield convert_numpy_types(final_result)

            print(f"ðŸŽ‰ ChatGPT ìŠ¤íƒ€ì¼ ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„ ì™„ë£Œ!")

        except Exception as e:
            import traceback
            print(f"âŒ Streaming analysis error: {str(e)}")
            print(f"Full traceback: {traceback.format_exc()}")

            yield {
                "type": "error",
                "content": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "step": "error"
            }

    async def _generate_simple_text_answer(self, question: str) -> str:
        """ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ë‹µë³€ ìƒì„± (ì½”ë“œ ì‹¤í–‰ ì—†ì´)"""
        try:
            prompt = f"""
ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì§ì ‘ì ì´ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

ìš”êµ¬ì‚¬í•­:
1. ëª…í™•í•˜ê³  ì •í™•í•œ ì •ë³´ ì œê³µ
2. í•„ìš”ì‹œ ë‹¨ê³„ë³„ ì„¤ëª…
3. ì‹¤ìš©ì ì¸ ì¡°ì–¸ì´ë‚˜ íŒ í¬í•¨
4. í•œêµ­ì–´ë¡œ ìžì—°ìŠ¤ëŸ½ê²Œ ìž‘ì„±
5. ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ êµ¬ì¡°í™”

ë‹µë³€:
"""

            response = await self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ìž…ë‹ˆë‹¤. ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=1500
            )

            return response.choices[0].message.content.strip()

        except Exception as e:
            print(f"ê°„ë‹¨ í…ìŠ¤íŠ¸ ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"


    async def _stream_code_generation(self, question: str, df=None, file_info=None):
        """ì‹¤ì‹œê°„ ì½”ë“œ ìƒì„± ìŠ¤íŠ¸ë¦¬ë°"""
        try:
            print(f"ðŸ”¥ ì‹¤ì‹œê°„ ì½”ë“œ ìƒì„± ì‹œìž‘: {question}")
            print(f"ðŸ“Š ë°ì´í„° ì •ë³´: {f'{len(df)}í–‰ {len(df.columns)}ì—´' if df is not None and not df.empty else 'ë°ì´í„° ì—†ìŒ'}")
            if df is not None and not df.empty:
                # ë°ì´í„° ì»¬ëŸ¼ ì •ë³´ ì¶”ê°€
                columns_info = ", ".join(df.columns.tolist())
                data_sample = df.head(3).to_string() if len(df) > 0 else "ë°ì´í„° ì—†ìŒ"

                prompt = f"""
ë°ì´í„° ë¶„ì„ì„ ìœ„í•œ Python ì½”ë“œë¥¼ ìž‘ì„±í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}
ë°ì´í„° ì •ë³´: {len(df)}í–‰ {len(df.columns)}ì—´
ì»¬ëŸ¼: {columns_info}

ì¤‘ìš”: ë°ì´í„°ëŠ” ì´ë¯¸ 'df' ë³€ìˆ˜ì— ë¡œë“œë˜ì–´ ìžˆìŠµë‹ˆë‹¤. pd.read_csv()ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.

ë°ì´í„° ìƒ˜í”Œ:
{data_sample}

ìš”êµ¬ì‚¬í•­:
1. ì´ë¯¸ ë¡œë“œëœ df ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶„ì„
2. plotlyë¥¼ ì‚¬ìš©í•œ ì‹œê°í™” í¬í•¨ (fig ë³€ìˆ˜ì— ì €ìž¥ í•„ìˆ˜)
3. ì™„ì „í•œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì½”ë“œ
4. print() ë¬¸ìœ¼ë¡œ ê²°ê³¼ ì¶œë ¥
5. í•œêµ­ì–´ ì£¼ì„ í¬í•¨
6. ë§ˆì§€ë§‰ì— ë°˜ë“œì‹œ ì´ ì½”ë“œ ì¶”ê°€:
   # NaN ê°’ ì²˜ë¦¬ í›„ ì°¨íŠ¸ JSON ìƒì„±
   import json
   import numpy as np
   try:
       chart_json = fig.to_json()
   except Exception as chart_error:
       print(f"Chart JSON generation error: {{chart_error}}")
       chart_json = None

Python ì½”ë“œë§Œ ë°˜í™˜í•˜ì„¸ìš”:
"""
            else:
                prompt = f"""
ì§ˆë¬¸: {question}

Python ê³„ì‚° ì½”ë“œë¥¼ ìž‘ì„±í•´ì£¼ì„¸ìš”.
- í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import
- ê³„ì‚° ë¡œì§ êµ¬í˜„
- ê°€ëŠ¥í•˜ë©´ plotly ì‹œê°í™” í¬í•¨
- ì™„ì „í•œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì½”ë“œ

Python ì½”ë“œë§Œ ë°˜í™˜í•˜ì„¸ìš”:
"""

            response = await self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ Python ì½”ë“œ ìƒì„± ì „ë¬¸ê°€ìž…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=1000,
                stream=True
            )

            generated_code = ""
            current_line = ""
            line_count = 0

            print("ðŸ”„ OpenAI ìŠ¤íŠ¸ë¦¬ë° ì‹œìž‘...")
            async for chunk in response:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    generated_code += content
                    current_line += content

                    # ì¤„ë°”ê¿ˆì´ ìžˆìœ¼ë©´ ì™„ì„±ëœ ë¼ì¸ë“¤ì„ yield
                    while '\n' in current_line:
                        line_break_index = current_line.find('\n')
                        completed_line = current_line[:line_break_index]

                        if completed_line.strip():  # ë¹ˆ ì¤„ì´ ì•„ë‹Œ ê²½ìš°ë§Œ
                            line_count += 1
                            print(f"ðŸ“ ì½”ë“œ ë¼ì¸ {line_count}: {completed_line[:50]}...")
                            yield {
                                "type": "code_line",
                                "content": completed_line
                            }
                            await asyncio.sleep(0.1)  # ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼

                        current_line = current_line[line_break_index + 1:]

            # ë§ˆì§€ë§‰ ì¤„ ì²˜ë¦¬
            if current_line.strip():
                yield {
                    "type": "code_line",
                    "content": current_line
                }

            # ì½”ë“œ ë¸”ë¡ ë§ˆì»¤ ì œê±°
            if "```python" in generated_code:
                generated_code = generated_code.split("```python")[1].split("```")[0]
            elif "```" in generated_code:
                generated_code = generated_code.split("```")[1].split("```")[0]

            yield {
                "type": "code_complete",
                "full_code": generated_code.strip()
            }

        except Exception as e:
            print(f"ì‹¤ì‹œê°„ ì½”ë“œ ìƒì„± ì˜¤ë¥˜: {e}")
            error_msg = str(e)
            yield {
                "type": "code_complete",
                "full_code": f"""
import pandas as pd
import plotly.express as px

print("ì½”ë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg}")
"""
            }

    async def _stream_comprehensive_answer(self, question: str, execution_result: str, insights: list, chart_data: dict = None):
        """ì¢…í•©ì ì¸ ë‹µë³€ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤ (ì¸ì‚¬ì´íŠ¸ í†µí•©)"""
        print(f"ðŸŽ¯ _stream_comprehensive_answer í•¨ìˆ˜ í˜¸ì¶œë¨")
        print(f"ðŸ“ question: {question[:50]}...")
        print(f"ðŸ“Š execution_result ê¸¸ì´: {len(execution_result) if execution_result else 0}")
        print(f"ðŸ”— chart_data: {'ìžˆìŒ' if chart_data else 'ì—†ìŒ'}")
        try:
            # ì‹¤í–‰ ê²°ê³¼ì—ì„œ ì‹¤ì œ ìˆ˜ì¹˜ ë°ì´í„° ì¶”ì¶œ
            actual_numbers = []
            if execution_result:
                import re
                numbers = re.findall(r'\d+[,.]?\d*', execution_result)
                actual_numbers = numbers[:10]  # ìµœëŒ€ 10ê°œ

            prompt = f"""
ì‚¬ìš©ìžì˜ ì§ˆë¬¸: {question}

Python ì½”ë“œ ì‹¤í–‰ ê²°ê³¼: {execution_result}

ì¶”ì¶œëœ ìˆ˜ì¹˜: {actual_numbers}

ì°¨íŠ¸ ë°ì´í„° ì—¬ë¶€: {'ìžˆìŒ' if chart_data else 'ì—†ìŒ'}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— ëŒ€í•œ ì¢…í•©ì ì´ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.

**ë‹µë³€ êµ¬ì„±:**
1. **ë¶„ì„ ê²°ê³¼ ìš”ì•½** - ì½”ë“œ ì‹¤í–‰ìœ¼ë¡œ ì–»ì€ í•µì‹¬ ê²°ê³¼
2. **ì£¼ìš” ë°œê²¬ì‚¬í•­** - ì‹¤ì œ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ êµ¬ì²´ì  ì¸ì‚¬ì´íŠ¸
3. **ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ì ì˜ í•´ì„** - ì‹¤ìš©ì  ì˜ë¯¸ì™€ í™œìš© ë°©ì•ˆ
4. **ê²°ë¡  ë° ì œì•ˆ** - í–¥í›„ í–‰ë™ ê³„íš

**ìž‘ì„± ì›ì¹™:**
- ì‹¤ì œ ì‹¤í–‰ ê²°ê³¼ì˜ êµ¬ì²´ì  ìˆ˜ì¹˜ë¥¼ ë°˜ë“œì‹œ ì–¸ê¸‰
- ì¼ë°˜ë¡ ì´ ì•„ë‹Œ ë°ì´í„° ê¸°ë°˜ì˜ êµ¬ì²´ì  ë¶„ì„
- ì¹œê·¼í•˜ê³  ì „ë¬¸ì ì¸ í†¤
- í•œêµ­ì–´ë¡œ ìž‘ì„±

ë‹µë³€ì„ ì‹œìž‘í•˜ì„¸ìš”:
"""

            print(f"ðŸš€ OpenAI API í˜¸ì¶œ ì‹œìž‘...")
            response = await self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. ë¶„ì„ ê²°ê³¼ë¥¼ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=500,
                stream=True
            )

            print(f"ðŸ“¡ OpenAI ìŠ¤íŠ¸ë¦¬ë° ì‹œìž‘...")
            chunk_count = 0
            async for chunk in response:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    chunk_count += 1
                    if chunk_count <= 3:  # ì²˜ìŒ 3ê°œ ì²­í¬ë§Œ ë¡œê·¸
                        print(f"ðŸ“ ì²­í¬ {chunk_count}: {content[:30]}...")
                    yield content
                    await asyncio.sleep(0.03)  # ChatGPT ìŠ¤íƒ€ì¼ íƒ€ì´í•‘ ì†ë„

            print(f"âœ… OpenAI ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ - ì´ {chunk_count}ê°œ ì²­í¬")

        except Exception as e:
            print(f"ì¢…í•© ë‹µë³€ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")
            yield "ë¶„ì„ ê²°ê³¼ë¥¼ ì •ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    async def _stream_simple_text_answer(self, question: str):
        """ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ë‹µë³€ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤"""
        try:
            response = await self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ìž…ë‹ˆë‹¤. ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì¹œê·¼í•˜ê²Œ ë‹µë³€í•©ë‹ˆë‹¤."},
                    {"role": "user", "content": question}
                ],
                temperature=0.7,
                max_tokens=300,
                stream=True
            )

            async for chunk in response:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    yield content
                    await asyncio.sleep(0.03)  # ChatGPT ìŠ¤íƒ€ì¼ íƒ€ì´í•‘ ì†ë„

        except Exception as e:
            print(f"í…ìŠ¤íŠ¸ ë‹µë³€ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")
            yield "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."


# AI ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
ai_service = AIService()